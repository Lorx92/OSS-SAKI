{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sF_9MAOKPyAY"
   },
   "source": [
    "Resume NER Part 4: Working with Flair NLP\n",
    "\n",
    "---\n",
    "\n",
    "In this part we will use flair NLP to train a model on our data and evaluate the results. Please make sure you have set up your Google account and uploaded your files to Google drive. This Notebook should run on Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXoVFeGlQdEu"
   },
   "source": [
    "Let's change the working directory to the Google drive where our training data is, and install flair nlp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29772,
     "status": "ok",
     "timestamp": 1560883622671,
     "user": {
      "displayName": "Daniel H.",
      "photoUrl": "",
      "userId": "10144954295787467059"
     },
     "user_tz": -120
    },
    "id": "MXiOU9ihIHvX",
    "outputId": "4467e719-da19-407b-97c6-e8295219d37f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "os.chdir(\"/content/gdrive/My Drive/SAKI/NER/flair\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1415
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50585,
     "status": "ok",
     "timestamp": 1560883643528,
     "user": {
      "displayName": "Daniel H.",
      "photoUrl": "",
      "userId": "10144954295787467059"
     },
     "user_tz": -120
    },
    "id": "l8542ZPSnM_d",
    "outputId": "f0c618b6-4c75-4ff3-f74b-1a20115a0435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/3a/2e777f65a71c1eaa259df44c44e39d7071ba8c7780a1564316a38bf86449/flair-0.4.2-py3-none-any.whl (136kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 47.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
      "Collecting sqlitedict>=1.6.0 (from flair)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/1c/c757b93147a219cf1e25cef7e1ad9b595b7f802159493c45ce116521caff/sqlitedict-1.6.0.tar.gz\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
      "Collecting mpld3==0.3 (from flair)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001b[K     |████████████████████████████████| 798kB 47.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.3)\n",
      "Collecting bpemb>=0.2.9 (from flair)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/70/468a9652095b370f797ed37ff77e742b11565c6fd79eaeca5f2e50b164a7/bpemb-0.3.0-py3-none-any.whl\n",
      "Collecting deprecated>=1.2.4 (from flair)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/7a/003fa432f1e45625626549726c2fbb7a29baa764e9d1fdb2323a5d779f8a/Deprecated-1.2.5-py2.py3-none-any.whl\n",
      "Collecting pytorch-pretrained-bert>=0.6.1 (from flair)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 51.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
      "Collecting regex (from flair)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
      "\u001b[K     |████████████████████████████████| 655kB 47.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.1.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.3)\n",
      "Collecting segtok>=1.5.7 (from flair)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
      "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.8.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.3.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.16.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.12.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
      "Collecting sentencepiece (from bpemb>=0.2.9->flair)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 49.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: wrapt<2,>=1 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.11.1)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (1.9.167)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.8.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.5.3)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.1.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (7.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (41.0.1)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.167 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (1.12.167)\n",
      "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->flair) (0.13.2)\n",
      "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.167->boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.14)\n",
      "Building wheels for collected packages: sqlitedict, mpld3, regex, segtok\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/57/d3/907c3ee02d35e66f674ad0106e61f06eeeb98f6ee66a6cc3fe\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
      "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
      "Successfully built sqlitedict mpld3 regex segtok\n",
      "Installing collected packages: sqlitedict, mpld3, sentencepiece, bpemb, deprecated, regex, pytorch-pretrained-bert, segtok, flair\n",
      "Successfully installed bpemb-0.3.0 deprecated-1.2.5 flair-0.4.2 mpld3-0.3 pytorch-pretrained-bert-0.6.2 regex-2019.6.8 segtok-1.5.7 sentencepiece-0.1.82 sqlitedict-1.6.0\n"
     ]
    }
   ],
   "source": [
    "# download flair library\n",
    "! pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOWwKlH8QwBU"
   },
   "source": [
    "In the next section, we will train a NER model with flair. This code is taken from the flair nlp tutorials section 7. \"Training a model\" \n",
    "https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58167,
     "status": "ok",
     "timestamp": 1560883651139,
     "user": {
      "displayName": "Daniel H.",
      "photoUrl": "",
      "userId": "10144954295787467059"
     },
     "user_tz": -120
    },
    "id": "jmj6vZ_AmD4c",
    "outputId": "c84a8aa9-2b2a-4539-9e2c-24891a92af35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 18:47:28,553 Reading data from /content/gdrive/My Drive/SAKI/NER/flair\n",
      "2019-06-18 18:47:28,554 Train: /content/gdrive/My Drive/SAKI/NER/flair/train_res_bilou.txt\n",
      "2019-06-18 18:47:28,560 Dev: None\n",
      "2019-06-18 18:47:28,561 Test: /content/gdrive/My Drive/SAKI/NER/flair/test_res_bilou.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:312: DeprecationWarning: Call to deprecated function (or staticmethod) read_column_data. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, column_format\n",
      "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:318: DeprecationWarning: Call to deprecated function (or staticmethod) read_column_data. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, column_format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 11569 train + 1285 dev + 3667 test sentences\n",
      "[b'<unk>', b'O', b'B-Name', b'L-Name', b'B-Companies_worked_at', b'I-Companies_worked_at', b'L-Companies_worked_at', b'U-Companies_worked_at', b'B-College_Name', b'L-College_Name', b'I-College_Name', b'-', b'U-College_Name', b'I-Name', b'<START>', b'<STOP>']\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from flair.datasets import Corpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "\n",
    "# folder where training and test data are\n",
    "data_folder = '/content/gdrive/My Drive/SAKI/NER/flair'\n",
    "\n",
    "train_file = 'train_res_bilou.txt'\n",
    "test_file = 'test_res_bilou.txt'\n",
    "\n",
    "# relevant columns for the \"gold standard\" in the bilou tagged corpus\n",
    "columns = {1: 'text', 3: 'ner'}\n",
    "\n",
    "# 1.0 is full data; use a smaller number like 0.1 to test run the code\n",
    "downsample = 1.0 \n",
    "\n",
    "corpus: Corpus = NLPTaskDataFetcher.load_column_corpus(\n",
    "    data_folder, columns, train_file=train_file, test_file=test_file, dev_file=None\n",
    ").downsample(downsample)\n",
    "print(corpus)\n",
    "\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type='ner')\n",
    "print(tag_dictionary.idx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 117524,
     "status": "ok",
     "timestamp": 1560883710524,
     "user": {
      "displayName": "Daniel H.",
      "photoUrl": "",
      "userId": "10144954295787467059"
     },
     "user_tz": -120
    },
    "id": "WaCfgVfr5C0O",
    "outputId": "365f9f54-4024-480d-c91f-45d72ade569e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 18:47:35,337 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpbgnfqaii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000128/160000128 [00:17<00:00, 9230471.05B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 18:47:53,693 copying /tmp/tmpbgnfqaii to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 18:47:53,913 removing temp file /tmp/tmpbgnfqaii\n",
      "2019-06-18 18:47:55,002 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/glove.gensim not found in cache, downloading to /tmp/tmpk3x7hji0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21494764/21494764 [00:03<00:00, 5444640.47B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 18:47:59,956 copying /tmp/tmpk3x7hji0 to cache at /root/.flair/embeddings/glove.gensim\n",
      "2019-06-18 18:47:59,980 removing temp file /tmp/tmpk3x7hji0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 18:48:02,668 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-forward--h2048-l1-d0.05-lr30-0.25-20/news-forward-0.4.1.pt not found in cache, downloading to /tmp/tmpcdwbdiu1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73034624/73034624 [00:09<00:00, 7887117.57B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 18:48:12,949 copying /tmp/tmpcdwbdiu1 to cache at /root/.flair/embeddings/news-forward-0.4.1.pt\n",
      "2019-06-18 18:48:13,019 removing temp file /tmp/tmpcdwbdiu1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 18:48:21,774 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/big-news-backward--h2048-l1-d0.05-lr30-0.25-20/news-backward-0.4.1.pt not found in cache, downloading to /tmp/tmp6d_mj2pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73034575/73034575 [00:09<00:00, 7709786.95B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 18:48:32,305 copying /tmp/tmp6d_mj2pt to cache at /root/.flair/embeddings/news-backward-0.4.1.pt\n",
      "2019-06-18 18:48:32,401 removing temp file /tmp/tmp6d_mj2pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize embeddings\n",
    "from flair.embeddings import TokenEmbeddings, CharacterEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings\n",
    "\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "    WordEmbeddings('glove'),\n",
    "    # CharacterEmbeddings(),\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(\n",
    "    hidden_size=256, embeddings=embeddings,\n",
    "    tag_dictionary=tag_dictionary, tag_type='ner',\n",
    "    use_crf=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10964
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10488155,
     "status": "ok",
     "timestamp": 1560897682655,
     "user": {
      "displayName": "Daniel H.",
      "photoUrl": "",
      "userId": "10144954295787467059"
     },
     "user_tz": -120
    },
    "id": "40giY_EA5C0R",
    "outputId": "c288dd8b-09b8-414b-ea97-02428d3f604b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-18 18:48:34,038 loading file resources/taggers/flair-wordflairembeddings-nodev/checkpoint.pt\n",
      "2019-06-18 18:48:39,209 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:48:39,218 Evaluation method: MICRO_F1_SCORE\n",
      "2019-06-18 18:48:41,144 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:48:42,576 epoch 30 - iter 0/402 - loss 0.19545156\n",
      "2019-06-18 18:49:10,527 epoch 30 - iter 40/402 - loss 0.19786211\n",
      "2019-06-18 18:49:45,029 epoch 30 - iter 80/402 - loss 0.21883249\n",
      "2019-06-18 18:50:12,880 epoch 30 - iter 120/402 - loss 0.20899170\n",
      "2019-06-18 18:50:44,409 epoch 30 - iter 160/402 - loss 0.21028733\n",
      "2019-06-18 18:51:13,363 epoch 30 - iter 200/402 - loss 0.21183648\n",
      "2019-06-18 18:51:41,159 epoch 30 - iter 240/402 - loss 0.20423505\n",
      "2019-06-18 18:52:11,540 epoch 30 - iter 280/402 - loss 0.20235777\n",
      "2019-06-18 18:52:42,389 epoch 30 - iter 320/402 - loss 0.20012768\n",
      "2019-06-18 18:53:11,844 epoch 30 - iter 360/402 - loss 0.19756627\n",
      "2019-06-18 18:53:37,172 epoch 30 - iter 400/402 - loss 0.19511478\n",
      "2019-06-18 18:53:38,314 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:53:38,316 EPOCH 30 done: loss 0.1949 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 18:55:03,833 TEST : loss 0.42618313431739807 - score 0.7152\n",
      "2019-06-18 18:55:09,166 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 18:55:10,514 epoch 31 - iter 0/402 - loss 0.04510990\n",
      "2019-06-18 18:55:41,481 epoch 31 - iter 40/402 - loss 0.24838415\n",
      "2019-06-18 18:56:09,206 epoch 31 - iter 80/402 - loss 0.20117100\n",
      "2019-06-18 18:56:41,742 epoch 31 - iter 120/402 - loss 0.19632218\n",
      "2019-06-18 18:57:10,883 epoch 31 - iter 160/402 - loss 0.19283805\n",
      "2019-06-18 18:57:41,948 epoch 31 - iter 200/402 - loss 0.19365828\n",
      "2019-06-18 18:58:07,635 epoch 31 - iter 240/402 - loss 0.19028347\n",
      "2019-06-18 18:58:40,255 epoch 31 - iter 280/402 - loss 0.19379348\n",
      "2019-06-18 18:59:08,040 epoch 31 - iter 320/402 - loss 0.19219066\n",
      "2019-06-18 18:59:37,885 epoch 31 - iter 360/402 - loss 0.19352028\n",
      "2019-06-18 19:00:07,337 epoch 31 - iter 400/402 - loss 0.19391727\n",
      "2019-06-18 19:00:08,588 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:00:08,590 EPOCH 31 done: loss 0.1939 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 19:01:34,286 TEST : loss 0.4008851647377014 - score 0.7204\n",
      "2019-06-18 19:01:39,507 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:01:41,931 epoch 32 - iter 0/402 - loss 0.28437194\n",
      "2019-06-18 19:02:13,222 epoch 32 - iter 40/402 - loss 0.14548726\n",
      "2019-06-18 19:02:41,277 epoch 32 - iter 80/402 - loss 0.17201443\n",
      "2019-06-18 19:03:13,398 epoch 32 - iter 120/402 - loss 0.18197815\n",
      "2019-06-18 19:03:41,663 epoch 32 - iter 160/402 - loss 0.18749057\n",
      "2019-06-18 19:04:08,985 epoch 32 - iter 200/402 - loss 0.18890157\n",
      "2019-06-18 19:04:38,434 epoch 32 - iter 240/402 - loss 0.19436164\n",
      "2019-06-18 19:05:09,505 epoch 32 - iter 280/402 - loss 0.19778808\n",
      "2019-06-18 19:05:35,585 epoch 32 - iter 320/402 - loss 0.19249245\n",
      "2019-06-18 19:06:08,272 epoch 32 - iter 360/402 - loss 0.19563981\n",
      "2019-06-18 19:06:35,627 epoch 32 - iter 400/402 - loss 0.19742324\n",
      "2019-06-18 19:06:36,814 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:06:36,816 EPOCH 32 done: loss 0.1973 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 19:08:02,612 TEST : loss 0.3683035671710968 - score 0.7404\n",
      "2019-06-18 19:08:07,886 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:08:09,483 epoch 33 - iter 0/402 - loss 0.37525609\n",
      "2019-06-18 19:08:39,170 epoch 33 - iter 40/402 - loss 0.17152578\n",
      "2019-06-18 19:09:08,592 epoch 33 - iter 80/402 - loss 0.21478504\n",
      "2019-06-18 19:09:39,260 epoch 33 - iter 120/402 - loss 0.21017963\n",
      "2019-06-18 19:10:08,507 epoch 33 - iter 160/402 - loss 0.20553644\n",
      "2019-06-18 19:10:35,953 epoch 33 - iter 200/402 - loss 0.20358472\n",
      "2019-06-18 19:11:06,933 epoch 33 - iter 240/402 - loss 0.20127983\n",
      "2019-06-18 19:11:36,307 epoch 33 - iter 280/402 - loss 0.19737756\n",
      "2019-06-18 19:12:08,301 epoch 33 - iter 320/402 - loss 0.18950083\n",
      "2019-06-18 19:12:38,528 epoch 33 - iter 360/402 - loss 0.18348936\n",
      "2019-06-18 19:13:07,824 epoch 33 - iter 400/402 - loss 0.18288812\n",
      "2019-06-18 19:13:09,727 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:13:09,728 EPOCH 33 done: loss 0.1826 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 19:14:35,613 TEST : loss 0.3887045979499817 - score 0.7428\n",
      "2019-06-18 19:14:41,547 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:14:43,554 epoch 34 - iter 0/402 - loss 0.24746042\n",
      "2019-06-18 19:15:13,208 epoch 34 - iter 40/402 - loss 0.13848776\n",
      "2019-06-18 19:15:40,237 epoch 34 - iter 80/402 - loss 0.14777387\n",
      "2019-06-18 19:16:06,788 epoch 34 - iter 120/402 - loss 0.14764500\n",
      "2019-06-18 19:16:32,378 epoch 34 - iter 160/402 - loss 0.14641297\n",
      "2019-06-18 19:17:03,286 epoch 34 - iter 200/402 - loss 0.15641330\n",
      "2019-06-18 19:17:33,373 epoch 34 - iter 240/402 - loss 0.15494281\n",
      "2019-06-18 19:18:02,326 epoch 34 - iter 280/402 - loss 0.15373147\n",
      "2019-06-18 19:18:34,283 epoch 34 - iter 320/402 - loss 0.15675731\n",
      "2019-06-18 19:19:04,243 epoch 34 - iter 360/402 - loss 0.16123427\n",
      "2019-06-18 19:19:36,199 epoch 34 - iter 400/402 - loss 0.16769571\n",
      "2019-06-18 19:19:38,032 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:19:38,033 EPOCH 34 done: loss 0.1694 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 19:21:03,956 TEST : loss 0.37406089901924133 - score 0.7322\n",
      "2019-06-18 19:21:09,064 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:21:10,590 epoch 35 - iter 0/402 - loss 0.14869013\n",
      "2019-06-18 19:21:40,560 epoch 35 - iter 40/402 - loss 0.22510169\n",
      "2019-06-18 19:22:12,550 epoch 35 - iter 80/402 - loss 0.22132297\n",
      "2019-06-18 19:22:43,879 epoch 35 - iter 120/402 - loss 0.21312326\n",
      "2019-06-18 19:23:14,031 epoch 35 - iter 160/402 - loss 0.19913920\n",
      "2019-06-18 19:23:41,465 epoch 35 - iter 200/402 - loss 0.18551365\n",
      "2019-06-18 19:24:13,570 epoch 35 - iter 240/402 - loss 0.18178054\n",
      "2019-06-18 19:24:44,772 epoch 35 - iter 280/402 - loss 0.18723427\n",
      "2019-06-18 19:25:10,530 epoch 35 - iter 320/402 - loss 0.19045502\n",
      "2019-06-18 19:25:36,751 epoch 35 - iter 360/402 - loss 0.18847434\n",
      "2019-06-18 19:26:07,152 epoch 35 - iter 400/402 - loss 0.18622886\n",
      "2019-06-18 19:26:08,465 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:26:08,468 EPOCH 35 done: loss 0.1860 - lr 0.1000 - bad epochs 0\n",
      "2019-06-18 19:27:32,720 TEST : loss 0.37724608182907104 - score 0.7399\n",
      "2019-06-18 19:27:38,673 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:27:39,790 epoch 36 - iter 0/402 - loss 0.00544026\n",
      "2019-06-18 19:28:10,965 epoch 36 - iter 40/402 - loss 0.17182140\n",
      "2019-06-18 19:28:40,496 epoch 36 - iter 80/402 - loss 0.19192872\n",
      "2019-06-18 19:29:10,995 epoch 36 - iter 120/402 - loss 0.18962597\n",
      "2019-06-18 19:29:44,299 epoch 36 - iter 160/402 - loss 0.19120840\n",
      "2019-06-18 19:30:14,455 epoch 36 - iter 200/402 - loss 0.18456095\n",
      "2019-06-18 19:30:43,438 epoch 36 - iter 240/402 - loss 0.18928295\n",
      "2019-06-18 19:31:10,815 epoch 36 - iter 280/402 - loss 0.18479401\n",
      "2019-06-18 19:31:37,630 epoch 36 - iter 320/402 - loss 0.18298937\n",
      "2019-06-18 19:32:07,476 epoch 36 - iter 360/402 - loss 0.18066125\n",
      "2019-06-18 19:32:36,926 epoch 36 - iter 400/402 - loss 0.17979766\n",
      "2019-06-18 19:32:38,449 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:32:38,453 EPOCH 36 done: loss 0.1808 - lr 0.1000 - bad epochs 1\n",
      "2019-06-18 19:34:04,548 TEST : loss 0.3831475079059601 - score 0.7413\n",
      "2019-06-18 19:34:09,728 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:34:10,743 epoch 37 - iter 0/402 - loss 0.23513353\n",
      "2019-06-18 19:34:38,403 epoch 37 - iter 40/402 - loss 0.15544937\n",
      "2019-06-18 19:35:09,390 epoch 37 - iter 80/402 - loss 0.17262794\n",
      "2019-06-18 19:35:39,316 epoch 37 - iter 120/402 - loss 0.17353328\n",
      "2019-06-18 19:36:10,661 epoch 37 - iter 160/402 - loss 0.17467111\n",
      "2019-06-18 19:36:38,870 epoch 37 - iter 200/402 - loss 0.16313725\n",
      "2019-06-18 19:37:13,216 epoch 37 - iter 240/402 - loss 0.16005445\n",
      "2019-06-18 19:37:40,823 epoch 37 - iter 280/402 - loss 0.15825373\n",
      "2019-06-18 19:38:08,575 epoch 37 - iter 320/402 - loss 0.16037985\n",
      "2019-06-18 19:38:34,886 epoch 37 - iter 360/402 - loss 0.16833132\n",
      "2019-06-18 19:39:05,911 epoch 37 - iter 400/402 - loss 0.17092440\n",
      "2019-06-18 19:39:07,813 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:39:07,816 EPOCH 37 done: loss 0.1713 - lr 0.1000 - bad epochs 2\n",
      "2019-06-18 19:40:32,132 TEST : loss 0.37875738739967346 - score 0.7237\n",
      "2019-06-18 19:40:37,529 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:40:38,678 epoch 38 - iter 0/402 - loss 0.03430226\n",
      "2019-06-18 19:41:12,358 epoch 38 - iter 40/402 - loss 0.17597473\n",
      "2019-06-18 19:41:39,100 epoch 38 - iter 80/402 - loss 0.15808669\n",
      "2019-06-18 19:42:07,030 epoch 38 - iter 120/402 - loss 0.15224948\n",
      "2019-06-18 19:42:37,052 epoch 38 - iter 160/402 - loss 0.16308542\n",
      "2019-06-18 19:43:10,099 epoch 38 - iter 200/402 - loss 0.17054030\n",
      "2019-06-18 19:43:39,636 epoch 38 - iter 240/402 - loss 0.16629552\n",
      "2019-06-18 19:44:07,965 epoch 38 - iter 280/402 - loss 0.16711996\n",
      "2019-06-18 19:44:39,169 epoch 38 - iter 320/402 - loss 0.17046477\n",
      "2019-06-18 19:45:08,007 epoch 38 - iter 360/402 - loss 0.17574487\n",
      "2019-06-18 19:45:34,626 epoch 38 - iter 400/402 - loss 0.17398730\n",
      "2019-06-18 19:45:35,968 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:45:35,970 EPOCH 38 done: loss 0.1747 - lr 0.1000 - bad epochs 3\n",
      "2019-06-18 19:47:02,064 TEST : loss 0.41906222701072693 - score 0.7142\n",
      "Epoch    37: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-06-18 19:47:07,357 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:47:08,774 epoch 39 - iter 0/402 - loss 0.02626225\n",
      "2019-06-18 19:47:41,351 epoch 39 - iter 40/402 - loss 0.18265364\n",
      "2019-06-18 19:48:08,746 epoch 39 - iter 80/402 - loss 0.19102409\n",
      "2019-06-18 19:48:36,291 epoch 39 - iter 120/402 - loss 0.18394970\n",
      "2019-06-18 19:49:05,736 epoch 39 - iter 160/402 - loss 0.18434209\n",
      "2019-06-18 19:49:35,803 epoch 39 - iter 200/402 - loss 0.17313592\n",
      "2019-06-18 19:50:06,103 epoch 39 - iter 240/402 - loss 0.16979805\n",
      "2019-06-18 19:50:37,413 epoch 39 - iter 280/402 - loss 0.16896736\n",
      "2019-06-18 19:51:06,362 epoch 39 - iter 320/402 - loss 0.16195962\n",
      "2019-06-18 19:51:35,991 epoch 39 - iter 360/402 - loss 0.15846833\n",
      "2019-06-18 19:52:04,467 epoch 39 - iter 400/402 - loss 0.15692697\n",
      "2019-06-18 19:52:05,750 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:52:05,753 EPOCH 39 done: loss 0.1568 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 19:53:29,804 TEST : loss 0.39653512835502625 - score 0.7278\n",
      "2019-06-18 19:53:35,180 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:53:36,427 epoch 40 - iter 0/402 - loss 0.07353616\n",
      "2019-06-18 19:54:09,418 epoch 40 - iter 40/402 - loss 0.17128331\n",
      "2019-06-18 19:54:41,643 epoch 40 - iter 80/402 - loss 0.17211494\n",
      "2019-06-18 19:55:11,487 epoch 40 - iter 120/402 - loss 0.16603038\n",
      "2019-06-18 19:55:44,965 epoch 40 - iter 160/402 - loss 0.16539653\n",
      "2019-06-18 19:56:11,192 epoch 40 - iter 200/402 - loss 0.15857145\n",
      "2019-06-18 19:56:40,996 epoch 40 - iter 240/402 - loss 0.15687170\n",
      "2019-06-18 19:57:09,974 epoch 40 - iter 280/402 - loss 0.15635175\n",
      "2019-06-18 19:57:36,483 epoch 40 - iter 320/402 - loss 0.15296725\n",
      "2019-06-18 19:58:05,000 epoch 40 - iter 360/402 - loss 0.15135462\n",
      "2019-06-18 19:58:34,017 epoch 40 - iter 400/402 - loss 0.15011164\n",
      "2019-06-18 19:58:35,490 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 19:58:35,492 EPOCH 40 done: loss 0.1503 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 20:00:01,682 TEST : loss 0.38952362537384033 - score 0.7356\n",
      "2019-06-18 20:00:07,256 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:00:09,046 epoch 41 - iter 0/402 - loss 0.27942887\n",
      "2019-06-18 20:00:38,612 epoch 41 - iter 40/402 - loss 0.12726484\n",
      "2019-06-18 20:01:07,178 epoch 41 - iter 80/402 - loss 0.13200863\n",
      "2019-06-18 20:01:35,197 epoch 41 - iter 120/402 - loss 0.12964028\n",
      "2019-06-18 20:02:04,828 epoch 41 - iter 160/402 - loss 0.12299558\n",
      "2019-06-18 20:02:37,283 epoch 41 - iter 200/402 - loss 0.13254822\n",
      "2019-06-18 20:03:08,068 epoch 41 - iter 240/402 - loss 0.13665368\n",
      "2019-06-18 20:03:40,063 epoch 41 - iter 280/402 - loss 0.13782869\n",
      "2019-06-18 20:04:07,954 epoch 41 - iter 320/402 - loss 0.14139124\n",
      "2019-06-18 20:04:34,339 epoch 41 - iter 360/402 - loss 0.13584928\n",
      "2019-06-18 20:05:06,578 epoch 41 - iter 400/402 - loss 0.13670759\n",
      "2019-06-18 20:05:07,888 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:05:07,889 EPOCH 41 done: loss 0.1364 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 20:06:32,047 TEST : loss 0.3952832520008087 - score 0.7265\n",
      "2019-06-18 20:06:37,199 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:06:38,399 epoch 42 - iter 0/402 - loss 0.02635014\n",
      "2019-06-18 20:07:11,920 epoch 42 - iter 40/402 - loss 0.11117669\n",
      "2019-06-18 20:07:41,310 epoch 42 - iter 80/402 - loss 0.12458662\n",
      "2019-06-18 20:08:09,372 epoch 42 - iter 120/402 - loss 0.13315342\n",
      "2019-06-18 20:08:38,274 epoch 42 - iter 160/402 - loss 0.14557741\n",
      "2019-06-18 20:09:08,428 epoch 42 - iter 200/402 - loss 0.14798749\n",
      "2019-06-18 20:09:36,399 epoch 42 - iter 240/402 - loss 0.14832609\n",
      "2019-06-18 20:10:07,318 epoch 42 - iter 280/402 - loss 0.14088387\n",
      "2019-06-18 20:10:35,520 epoch 42 - iter 320/402 - loss 0.14022242\n",
      "2019-06-18 20:11:09,764 epoch 42 - iter 360/402 - loss 0.14286729\n",
      "2019-06-18 20:11:35,265 epoch 42 - iter 400/402 - loss 0.14466974\n",
      "2019-06-18 20:11:36,806 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:11:36,808 EPOCH 42 done: loss 0.1455 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 20:13:02,664 TEST : loss 0.39371010661125183 - score 0.7294\n",
      "2019-06-18 20:13:07,874 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:13:09,336 epoch 43 - iter 0/402 - loss 0.08549699\n",
      "2019-06-18 20:13:38,356 epoch 43 - iter 40/402 - loss 0.15251545\n",
      "2019-06-18 20:14:05,351 epoch 43 - iter 80/402 - loss 0.13523451\n",
      "2019-06-18 20:14:38,362 epoch 43 - iter 120/402 - loss 0.14839326\n",
      "2019-06-18 20:15:07,834 epoch 43 - iter 160/402 - loss 0.14522154\n",
      "2019-06-18 20:15:35,031 epoch 43 - iter 200/402 - loss 0.13910848\n",
      "2019-06-18 20:16:05,318 epoch 43 - iter 240/402 - loss 0.14182757\n",
      "2019-06-18 20:16:31,526 epoch 43 - iter 280/402 - loss 0.14147054\n",
      "2019-06-18 20:17:02,133 epoch 43 - iter 320/402 - loss 0.14071974\n",
      "2019-06-18 20:17:33,855 epoch 43 - iter 360/402 - loss 0.14023373\n",
      "2019-06-18 20:18:05,016 epoch 43 - iter 400/402 - loss 0.14311278\n",
      "2019-06-18 20:18:06,341 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:18:06,342 EPOCH 43 done: loss 0.1435 - lr 0.0500 - bad epochs 1\n",
      "2019-06-18 20:19:32,480 TEST : loss 0.3907146155834198 - score 0.7346\n",
      "2019-06-18 20:19:37,743 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:19:39,358 epoch 44 - iter 0/402 - loss 0.06848058\n",
      "2019-06-18 20:20:10,372 epoch 44 - iter 40/402 - loss 0.11949165\n",
      "2019-06-18 20:20:41,557 epoch 44 - iter 80/402 - loss 0.14130557\n",
      "2019-06-18 20:21:12,932 epoch 44 - iter 120/402 - loss 0.14694822\n",
      "2019-06-18 20:21:43,387 epoch 44 - iter 160/402 - loss 0.13762633\n",
      "2019-06-18 20:22:16,291 epoch 44 - iter 200/402 - loss 0.13607561\n",
      "2019-06-18 20:22:41,851 epoch 44 - iter 240/402 - loss 0.13403577\n",
      "2019-06-18 20:23:08,066 epoch 44 - iter 280/402 - loss 0.13560312\n",
      "2019-06-18 20:23:37,828 epoch 44 - iter 320/402 - loss 0.13674407\n",
      "2019-06-18 20:24:11,222 epoch 44 - iter 360/402 - loss 0.13644146\n",
      "2019-06-18 20:24:41,537 epoch 44 - iter 400/402 - loss 0.13405416\n",
      "2019-06-18 20:24:42,854 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:24:42,858 EPOCH 44 done: loss 0.1347 - lr 0.0500 - bad epochs 2\n",
      "2019-06-18 20:26:08,923 TEST : loss 0.39963459968566895 - score 0.7277\n",
      "2019-06-18 20:26:14,101 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:26:16,173 epoch 45 - iter 0/402 - loss 0.23526376\n",
      "2019-06-18 20:26:47,436 epoch 45 - iter 40/402 - loss 0.10698834\n",
      "2019-06-18 20:27:15,892 epoch 45 - iter 80/402 - loss 0.12453665\n",
      "2019-06-18 20:27:48,316 epoch 45 - iter 120/402 - loss 0.12739809\n",
      "2019-06-18 20:28:20,202 epoch 45 - iter 160/402 - loss 0.14534688\n",
      "2019-06-18 20:28:46,990 epoch 45 - iter 200/402 - loss 0.14082752\n",
      "2019-06-18 20:29:16,780 epoch 45 - iter 240/402 - loss 0.13407535\n",
      "2019-06-18 20:29:46,401 epoch 45 - iter 280/402 - loss 0.13388740\n",
      "2019-06-18 20:30:13,606 epoch 45 - iter 320/402 - loss 0.13539691\n",
      "2019-06-18 20:30:39,763 epoch 45 - iter 360/402 - loss 0.13267154\n",
      "2019-06-18 20:31:10,329 epoch 45 - iter 400/402 - loss 0.13301462\n",
      "2019-06-18 20:31:11,845 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:31:11,846 EPOCH 45 done: loss 0.1329 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 20:32:36,010 TEST : loss 0.39769938588142395 - score 0.7203\n",
      "2019-06-18 20:32:41,269 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:32:43,356 epoch 46 - iter 0/402 - loss 0.04495692\n",
      "2019-06-18 20:33:16,660 epoch 46 - iter 40/402 - loss 0.13955206\n",
      "2019-06-18 20:33:47,068 epoch 46 - iter 80/402 - loss 0.11257169\n",
      "2019-06-18 20:34:15,121 epoch 46 - iter 120/402 - loss 0.12267051\n",
      "2019-06-18 20:34:49,716 epoch 46 - iter 160/402 - loss 0.13350317\n",
      "2019-06-18 20:35:17,012 epoch 46 - iter 200/402 - loss 0.13713186\n",
      "2019-06-18 20:35:49,147 epoch 46 - iter 240/402 - loss 0.13479152\n",
      "2019-06-18 20:36:17,158 epoch 46 - iter 280/402 - loss 0.13273098\n",
      "2019-06-18 20:36:48,555 epoch 46 - iter 320/402 - loss 0.12969920\n",
      "2019-06-18 20:37:13,953 epoch 46 - iter 360/402 - loss 0.12845602\n",
      "2019-06-18 20:37:40,487 epoch 46 - iter 400/402 - loss 0.12938856\n",
      "2019-06-18 20:37:41,985 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:37:41,987 EPOCH 46 done: loss 0.1293 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 20:39:07,965 TEST : loss 0.39893022179603577 - score 0.735\n",
      "2019-06-18 20:39:13,220 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:39:14,529 epoch 47 - iter 0/402 - loss 0.01999795\n",
      "2019-06-18 20:39:43,687 epoch 47 - iter 40/402 - loss 0.10562224\n",
      "2019-06-18 20:40:14,672 epoch 47 - iter 80/402 - loss 0.11053515\n",
      "2019-06-18 20:40:41,935 epoch 47 - iter 120/402 - loss 0.10871328\n",
      "2019-06-18 20:41:11,734 epoch 47 - iter 160/402 - loss 0.11624246\n",
      "2019-06-18 20:41:38,144 epoch 47 - iter 200/402 - loss 0.11769139\n",
      "2019-06-18 20:42:09,605 epoch 47 - iter 240/402 - loss 0.12482532\n",
      "2019-06-18 20:42:39,067 epoch 47 - iter 280/402 - loss 0.12245545\n",
      "2019-06-18 20:43:06,614 epoch 47 - iter 320/402 - loss 0.12216468\n",
      "2019-06-18 20:43:35,371 epoch 47 - iter 360/402 - loss 0.12199008\n",
      "2019-06-18 20:44:07,751 epoch 47 - iter 400/402 - loss 0.12194691\n",
      "2019-06-18 20:44:09,229 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:44:09,231 EPOCH 47 done: loss 0.1218 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 20:45:35,228 TEST : loss 0.3989253044128418 - score 0.7378\n",
      "2019-06-18 20:45:40,613 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:45:41,860 epoch 48 - iter 0/402 - loss 0.00308561\n",
      "2019-06-18 20:46:11,213 epoch 48 - iter 40/402 - loss 0.10523015\n",
      "2019-06-18 20:46:42,355 epoch 48 - iter 80/402 - loss 0.10395974\n",
      "2019-06-18 20:47:11,844 epoch 48 - iter 120/402 - loss 0.10879788\n",
      "2019-06-18 20:47:43,572 epoch 48 - iter 160/402 - loss 0.10503434\n",
      "2019-06-18 20:48:11,427 epoch 48 - iter 200/402 - loss 0.10849980\n",
      "2019-06-18 20:48:43,119 epoch 48 - iter 240/402 - loss 0.11670236\n",
      "2019-06-18 20:49:11,086 epoch 48 - iter 280/402 - loss 0.12290531\n",
      "2019-06-18 20:49:39,319 epoch 48 - iter 320/402 - loss 0.12524173\n",
      "2019-06-18 20:50:04,466 epoch 48 - iter 360/402 - loss 0.12438371\n",
      "2019-06-18 20:50:31,717 epoch 48 - iter 400/402 - loss 0.12420638\n",
      "2019-06-18 20:50:33,251 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:50:33,253 EPOCH 48 done: loss 0.1241 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 20:51:59,383 TEST : loss 0.40950414538383484 - score 0.7211\n",
      "2019-06-18 20:52:04,689 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:52:05,935 epoch 49 - iter 0/402 - loss 0.06629434\n",
      "2019-06-18 20:52:33,798 epoch 49 - iter 40/402 - loss 0.10509352\n",
      "2019-06-18 20:53:02,530 epoch 49 - iter 80/402 - loss 0.11103899\n",
      "2019-06-18 20:53:33,178 epoch 49 - iter 120/402 - loss 0.11084694\n",
      "2019-06-18 20:54:04,660 epoch 49 - iter 160/402 - loss 0.12070368\n",
      "2019-06-18 20:54:34,104 epoch 49 - iter 200/402 - loss 0.12380713\n",
      "2019-06-18 20:55:04,180 epoch 49 - iter 240/402 - loss 0.12382488\n",
      "2019-06-18 20:55:31,851 epoch 49 - iter 280/402 - loss 0.12369405\n",
      "2019-06-18 20:56:03,621 epoch 49 - iter 320/402 - loss 0.12579628\n",
      "2019-06-18 20:56:28,659 epoch 49 - iter 360/402 - loss 0.12148978\n",
      "2019-06-18 20:56:59,236 epoch 49 - iter 400/402 - loss 0.12368786\n",
      "2019-06-18 20:57:00,656 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:57:00,658 EPOCH 49 done: loss 0.1236 - lr 0.0500 - bad epochs 1\n",
      "2019-06-18 20:58:26,707 TEST : loss 0.393038272857666 - score 0.7364\n",
      "2019-06-18 20:58:32,044 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 20:58:33,492 epoch 50 - iter 0/402 - loss 0.07655984\n",
      "2019-06-18 20:59:04,467 epoch 50 - iter 40/402 - loss 0.10882244\n",
      "2019-06-18 20:59:36,755 epoch 50 - iter 80/402 - loss 0.13539010\n",
      "2019-06-18 21:00:04,787 epoch 50 - iter 120/402 - loss 0.12674597\n",
      "2019-06-18 21:00:33,834 epoch 50 - iter 160/402 - loss 0.11780192\n",
      "2019-06-18 21:01:03,735 epoch 50 - iter 200/402 - loss 0.11594065\n",
      "2019-06-18 21:01:32,902 epoch 50 - iter 240/402 - loss 0.11490535\n",
      "2019-06-18 21:02:03,567 epoch 50 - iter 280/402 - loss 0.11585825\n",
      "2019-06-18 21:02:32,162 epoch 50 - iter 320/402 - loss 0.12184833\n",
      "2019-06-18 21:03:03,474 epoch 50 - iter 360/402 - loss 0.12338648\n",
      "2019-06-18 21:03:29,293 epoch 50 - iter 400/402 - loss 0.12186239\n",
      "2019-06-18 21:03:30,518 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:03:30,521 EPOCH 50 done: loss 0.1216 - lr 0.0500 - bad epochs 2\n",
      "2019-06-18 21:04:56,691 TEST : loss 0.3928367793560028 - score 0.7309\n",
      "2019-06-18 21:05:01,998 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:05:03,201 epoch 51 - iter 0/402 - loss 0.02072158\n",
      "2019-06-18 21:05:34,344 epoch 51 - iter 40/402 - loss 0.09821156\n",
      "2019-06-18 21:06:03,006 epoch 51 - iter 80/402 - loss 0.09910345\n",
      "2019-06-18 21:06:30,262 epoch 51 - iter 120/402 - loss 0.10701154\n",
      "2019-06-18 21:07:00,061 epoch 51 - iter 160/402 - loss 0.11238745\n",
      "2019-06-18 21:07:27,247 epoch 51 - iter 200/402 - loss 0.11428858\n",
      "2019-06-18 21:07:59,263 epoch 51 - iter 240/402 - loss 0.11843854\n",
      "2019-06-18 21:08:27,700 epoch 51 - iter 280/402 - loss 0.12348365\n",
      "2019-06-18 21:08:56,731 epoch 51 - iter 320/402 - loss 0.12283057\n",
      "2019-06-18 21:09:25,791 epoch 51 - iter 360/402 - loss 0.12309253\n",
      "2019-06-18 21:10:00,699 epoch 51 - iter 400/402 - loss 0.12177055\n",
      "2019-06-18 21:10:03,030 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:10:03,039 EPOCH 51 done: loss 0.1219 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 21:11:29,067 TEST : loss 0.41766753792762756 - score 0.7351\n",
      "2019-06-18 21:11:34,268 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:11:35,668 epoch 52 - iter 0/402 - loss 0.02440697\n",
      "2019-06-18 21:12:10,440 epoch 52 - iter 40/402 - loss 0.13242179\n",
      "2019-06-18 21:12:37,319 epoch 52 - iter 80/402 - loss 0.12633269\n",
      "2019-06-18 21:13:05,651 epoch 52 - iter 120/402 - loss 0.12113669\n",
      "2019-06-18 21:13:41,502 epoch 52 - iter 160/402 - loss 0.12558627\n",
      "2019-06-18 21:14:08,806 epoch 52 - iter 200/402 - loss 0.11974541\n",
      "2019-06-18 21:14:37,413 epoch 52 - iter 240/402 - loss 0.11967954\n",
      "2019-06-18 21:15:07,160 epoch 52 - iter 280/402 - loss 0.11744358\n",
      "2019-06-18 21:15:37,126 epoch 52 - iter 320/402 - loss 0.11809779\n",
      "2019-06-18 21:16:06,040 epoch 52 - iter 360/402 - loss 0.12019483\n",
      "2019-06-18 21:16:31,982 epoch 52 - iter 400/402 - loss 0.11662091\n",
      "2019-06-18 21:16:33,188 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:16:33,189 EPOCH 52 done: loss 0.1163 - lr 0.0500 - bad epochs 1\n",
      "2019-06-18 21:17:59,268 TEST : loss 0.4035005271434784 - score 0.7181\n",
      "2019-06-18 21:18:04,410 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:18:05,685 epoch 53 - iter 0/402 - loss 0.10834166\n",
      "2019-06-18 21:18:35,079 epoch 53 - iter 40/402 - loss 0.12487769\n",
      "2019-06-18 21:19:09,485 epoch 53 - iter 80/402 - loss 0.11720757\n",
      "2019-06-18 21:19:41,129 epoch 53 - iter 120/402 - loss 0.11425325\n",
      "2019-06-18 21:20:07,483 epoch 53 - iter 160/402 - loss 0.10733500\n",
      "2019-06-18 21:20:37,136 epoch 53 - iter 200/402 - loss 0.11402323\n",
      "2019-06-18 21:21:04,983 epoch 53 - iter 240/402 - loss 0.11140607\n",
      "2019-06-18 21:21:33,704 epoch 53 - iter 280/402 - loss 0.11333858\n",
      "2019-06-18 21:22:04,311 epoch 53 - iter 320/402 - loss 0.11861804\n",
      "2019-06-18 21:22:35,775 epoch 53 - iter 360/402 - loss 0.12083370\n",
      "2019-06-18 21:23:03,621 epoch 53 - iter 400/402 - loss 0.12340167\n",
      "2019-06-18 21:23:05,147 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:23:05,149 EPOCH 53 done: loss 0.1231 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 21:24:31,074 TEST : loss 0.3983229398727417 - score 0.7409\n",
      "2019-06-18 21:24:36,210 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:24:37,883 epoch 54 - iter 0/402 - loss 0.02751890\n",
      "2019-06-18 21:25:06,383 epoch 54 - iter 40/402 - loss 0.11839231\n",
      "2019-06-18 21:25:35,662 epoch 54 - iter 80/402 - loss 0.13467936\n",
      "2019-06-18 21:26:04,518 epoch 54 - iter 120/402 - loss 0.12599056\n",
      "2019-06-18 21:26:36,459 epoch 54 - iter 160/402 - loss 0.12013425\n",
      "2019-06-18 21:27:06,844 epoch 54 - iter 200/402 - loss 0.11625840\n",
      "2019-06-18 21:27:35,962 epoch 54 - iter 240/402 - loss 0.11942145\n",
      "2019-06-18 21:28:05,369 epoch 54 - iter 280/402 - loss 0.11501082\n",
      "2019-06-18 21:28:35,696 epoch 54 - iter 320/402 - loss 0.11684575\n",
      "2019-06-18 21:29:05,968 epoch 54 - iter 360/402 - loss 0.11580205\n",
      "2019-06-18 21:29:33,790 epoch 54 - iter 400/402 - loss 0.11396555\n",
      "2019-06-18 21:29:35,331 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:29:35,340 EPOCH 54 done: loss 0.1141 - lr 0.0500 - bad epochs 1\n",
      "2019-06-18 21:31:01,373 TEST : loss 0.4155767560005188 - score 0.7294\n",
      "2019-06-18 21:31:06,797 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:31:08,273 epoch 55 - iter 0/402 - loss 0.08471733\n",
      "2019-06-18 21:31:40,275 epoch 55 - iter 40/402 - loss 0.13525993\n",
      "2019-06-18 21:32:12,791 epoch 55 - iter 80/402 - loss 0.12771664\n",
      "2019-06-18 21:32:44,911 epoch 55 - iter 120/402 - loss 0.13759105\n",
      "2019-06-18 21:33:12,987 epoch 55 - iter 160/402 - loss 0.13199617\n",
      "2019-06-18 21:33:41,575 epoch 55 - iter 200/402 - loss 0.13016686\n",
      "2019-06-18 21:34:10,935 epoch 55 - iter 240/402 - loss 0.12778024\n",
      "2019-06-18 21:34:39,517 epoch 55 - iter 280/402 - loss 0.12402442\n",
      "2019-06-18 21:35:04,495 epoch 55 - iter 320/402 - loss 0.11986542\n",
      "2019-06-18 21:35:33,153 epoch 55 - iter 360/402 - loss 0.11576252\n",
      "2019-06-18 21:36:06,124 epoch 55 - iter 400/402 - loss 0.11473178\n",
      "2019-06-18 21:36:07,312 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:36:07,313 EPOCH 55 done: loss 0.1146 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 21:37:33,073 TEST : loss 0.3975781202316284 - score 0.734\n",
      "2019-06-18 21:37:38,312 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:37:39,453 epoch 56 - iter 0/402 - loss 0.10030347\n",
      "2019-06-18 21:38:08,152 epoch 56 - iter 40/402 - loss 0.13134781\n",
      "2019-06-18 21:38:40,357 epoch 56 - iter 80/402 - loss 0.12890638\n",
      "2019-06-18 21:39:14,593 epoch 56 - iter 120/402 - loss 0.13136132\n",
      "2019-06-18 21:39:45,768 epoch 56 - iter 160/402 - loss 0.12713488\n",
      "2019-06-18 21:40:13,352 epoch 56 - iter 200/402 - loss 0.12342332\n",
      "2019-06-18 21:40:40,041 epoch 56 - iter 240/402 - loss 0.11403702\n",
      "2019-06-18 21:41:10,600 epoch 56 - iter 280/402 - loss 0.11173086\n",
      "2019-06-18 21:41:34,460 epoch 56 - iter 320/402 - loss 0.11052956\n",
      "2019-06-18 21:42:03,660 epoch 56 - iter 360/402 - loss 0.11551986\n",
      "2019-06-18 21:42:34,956 epoch 56 - iter 400/402 - loss 0.11072359\n",
      "2019-06-18 21:42:36,448 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:42:36,449 EPOCH 56 done: loss 0.1114 - lr 0.0500 - bad epochs 1\n",
      "2019-06-18 21:44:02,618 TEST : loss 0.4189085364341736 - score 0.732\n",
      "2019-06-18 21:44:08,024 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:44:09,288 epoch 57 - iter 0/402 - loss 0.25347820\n",
      "2019-06-18 21:44:38,919 epoch 57 - iter 40/402 - loss 0.12829008\n",
      "2019-06-18 21:45:10,267 epoch 57 - iter 80/402 - loss 0.11465965\n",
      "2019-06-18 21:45:39,180 epoch 57 - iter 120/402 - loss 0.10856806\n",
      "2019-06-18 21:46:09,855 epoch 57 - iter 160/402 - loss 0.10602617\n",
      "2019-06-18 21:46:40,850 epoch 57 - iter 200/402 - loss 0.11062633\n",
      "2019-06-18 21:47:09,533 epoch 57 - iter 240/402 - loss 0.11251101\n",
      "2019-06-18 21:47:39,574 epoch 57 - iter 280/402 - loss 0.11510624\n",
      "2019-06-18 21:48:04,986 epoch 57 - iter 320/402 - loss 0.11353735\n",
      "2019-06-18 21:48:34,843 epoch 57 - iter 360/402 - loss 0.11285043\n",
      "2019-06-18 21:49:05,665 epoch 57 - iter 400/402 - loss 0.11408456\n",
      "2019-06-18 21:49:06,959 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:49:06,962 EPOCH 57 done: loss 0.1138 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 21:50:32,856 TEST : loss 0.41087356209754944 - score 0.7343\n",
      "2019-06-18 21:50:38,219 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:50:39,569 epoch 58 - iter 0/402 - loss 0.10612017\n",
      "2019-06-18 21:51:09,168 epoch 58 - iter 40/402 - loss 0.11870147\n",
      "2019-06-18 21:51:39,119 epoch 58 - iter 80/402 - loss 0.12179194\n",
      "2019-06-18 21:52:08,640 epoch 58 - iter 120/402 - loss 0.12551409\n",
      "2019-06-18 21:52:40,031 epoch 58 - iter 160/402 - loss 0.11907979\n",
      "2019-06-18 21:53:10,897 epoch 58 - iter 200/402 - loss 0.11921596\n",
      "2019-06-18 21:53:40,998 epoch 58 - iter 240/402 - loss 0.11398883\n",
      "2019-06-18 21:54:09,240 epoch 58 - iter 280/402 - loss 0.11224670\n",
      "2019-06-18 21:54:38,762 epoch 58 - iter 320/402 - loss 0.11125816\n",
      "2019-06-18 21:55:05,985 epoch 58 - iter 360/402 - loss 0.11051621\n",
      "2019-06-18 21:55:36,351 epoch 58 - iter 400/402 - loss 0.11200640\n",
      "2019-06-18 21:55:38,053 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:55:38,060 EPOCH 58 done: loss 0.1120 - lr 0.0500 - bad epochs 1\n",
      "2019-06-18 21:57:04,107 TEST : loss 0.4052702486515045 - score 0.7316\n",
      "2019-06-18 21:57:09,454 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 21:57:10,925 epoch 59 - iter 0/402 - loss 0.02759093\n",
      "2019-06-18 21:57:41,081 epoch 59 - iter 40/402 - loss 0.09338251\n",
      "2019-06-18 21:58:14,868 epoch 59 - iter 80/402 - loss 0.11084099\n",
      "2019-06-18 21:58:40,072 epoch 59 - iter 120/402 - loss 0.10664793\n",
      "2019-06-18 21:59:09,701 epoch 59 - iter 160/402 - loss 0.10778522\n",
      "2019-06-18 21:59:39,075 epoch 59 - iter 200/402 - loss 0.11017042\n",
      "2019-06-18 22:00:10,037 epoch 59 - iter 240/402 - loss 0.10629879\n",
      "2019-06-18 22:00:41,086 epoch 59 - iter 280/402 - loss 0.11231475\n",
      "2019-06-18 22:01:12,178 epoch 59 - iter 320/402 - loss 0.11136583\n",
      "2019-06-18 22:01:40,290 epoch 59 - iter 360/402 - loss 0.10745039\n",
      "2019-06-18 22:02:11,277 epoch 59 - iter 400/402 - loss 0.10510817\n",
      "2019-06-18 22:02:12,854 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:02:12,856 EPOCH 59 done: loss 0.1053 - lr 0.0500 - bad epochs 2\n",
      "2019-06-18 22:03:38,986 TEST : loss 0.44175055623054504 - score 0.729\n",
      "2019-06-18 22:03:44,299 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:03:45,457 epoch 60 - iter 0/402 - loss 0.00295472\n",
      "2019-06-18 22:04:13,196 epoch 60 - iter 40/402 - loss 0.10970632\n",
      "2019-06-18 22:04:39,640 epoch 60 - iter 80/402 - loss 0.08801627\n",
      "2019-06-18 22:05:09,997 epoch 60 - iter 120/402 - loss 0.09332557\n",
      "2019-06-18 22:05:40,775 epoch 60 - iter 160/402 - loss 0.09930522\n",
      "2019-06-18 22:06:13,095 epoch 60 - iter 200/402 - loss 0.10640139\n",
      "2019-06-18 22:06:42,932 epoch 60 - iter 240/402 - loss 0.11169029\n",
      "2019-06-18 22:07:11,923 epoch 60 - iter 280/402 - loss 0.11217957\n",
      "2019-06-18 22:07:38,546 epoch 60 - iter 320/402 - loss 0.11034638\n",
      "2019-06-18 22:08:05,144 epoch 60 - iter 360/402 - loss 0.10839101\n",
      "2019-06-18 22:08:37,756 epoch 60 - iter 400/402 - loss 0.10905683\n",
      "2019-06-18 22:08:39,049 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:08:39,051 EPOCH 60 done: loss 0.1093 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 22:10:05,121 TEST : loss 0.42993006110191345 - score 0.7248\n",
      "2019-06-18 22:10:10,483 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:10:12,621 epoch 61 - iter 0/402 - loss 0.00532776\n",
      "2019-06-18 22:10:43,755 epoch 61 - iter 40/402 - loss 0.08294948\n",
      "2019-06-18 22:11:15,325 epoch 61 - iter 80/402 - loss 0.09790291\n",
      "2019-06-18 22:11:42,661 epoch 61 - iter 120/402 - loss 0.09867064\n",
      "2019-06-18 22:12:12,694 epoch 61 - iter 160/402 - loss 0.10539575\n",
      "2019-06-18 22:12:43,267 epoch 61 - iter 200/402 - loss 0.10518092\n",
      "2019-06-18 22:13:12,849 epoch 61 - iter 240/402 - loss 0.10670475\n",
      "2019-06-18 22:13:41,935 epoch 61 - iter 280/402 - loss 0.10267207\n",
      "2019-06-18 22:14:10,166 epoch 61 - iter 320/402 - loss 0.10403647\n",
      "2019-06-18 22:14:38,959 epoch 61 - iter 360/402 - loss 0.10276194\n",
      "2019-06-18 22:15:09,543 epoch 61 - iter 400/402 - loss 0.10690115\n",
      "2019-06-18 22:15:10,777 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:15:10,780 EPOCH 61 done: loss 0.1072 - lr 0.0500 - bad epochs 1\n",
      "2019-06-18 22:16:36,694 TEST : loss 0.43448927998542786 - score 0.7284\n",
      "2019-06-18 22:16:42,027 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:16:43,583 epoch 62 - iter 0/402 - loss 0.08617213\n",
      "2019-06-18 22:17:12,676 epoch 62 - iter 40/402 - loss 0.09978127\n",
      "2019-06-18 22:17:42,665 epoch 62 - iter 80/402 - loss 0.10583954\n",
      "2019-06-18 22:18:12,956 epoch 62 - iter 120/402 - loss 0.10406685\n",
      "2019-06-18 22:18:41,103 epoch 62 - iter 160/402 - loss 0.09795066\n",
      "2019-06-18 22:19:06,211 epoch 62 - iter 200/402 - loss 0.10177200\n",
      "2019-06-18 22:19:33,614 epoch 62 - iter 240/402 - loss 0.10332830\n",
      "2019-06-18 22:20:05,893 epoch 62 - iter 280/402 - loss 0.10289378\n",
      "2019-06-18 22:20:37,190 epoch 62 - iter 320/402 - loss 0.10512725\n",
      "2019-06-18 22:21:09,853 epoch 62 - iter 360/402 - loss 0.10659870\n",
      "2019-06-18 22:21:38,159 epoch 62 - iter 400/402 - loss 0.10453099\n",
      "2019-06-18 22:21:39,465 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:21:39,467 EPOCH 62 done: loss 0.1044 - lr 0.0500 - bad epochs 2\n",
      "2019-06-18 22:23:05,507 TEST : loss 0.4230072796344757 - score 0.727\n",
      "2019-06-18 22:23:10,744 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:23:12,156 epoch 63 - iter 0/402 - loss 0.02416644\n",
      "2019-06-18 22:23:40,356 epoch 63 - iter 40/402 - loss 0.11973353\n",
      "2019-06-18 22:24:13,127 epoch 63 - iter 80/402 - loss 0.11241546\n",
      "2019-06-18 22:24:46,526 epoch 63 - iter 120/402 - loss 0.12064660\n",
      "2019-06-18 22:25:14,787 epoch 63 - iter 160/402 - loss 0.11311194\n",
      "2019-06-18 22:25:43,746 epoch 63 - iter 200/402 - loss 0.11033652\n",
      "2019-06-18 22:26:11,808 epoch 63 - iter 240/402 - loss 0.10453005\n",
      "2019-06-18 22:26:39,394 epoch 63 - iter 280/402 - loss 0.10107112\n",
      "2019-06-18 22:27:08,532 epoch 63 - iter 320/402 - loss 0.09644087\n",
      "2019-06-18 22:27:41,764 epoch 63 - iter 360/402 - loss 0.09882081\n",
      "2019-06-18 22:28:08,287 epoch 63 - iter 400/402 - loss 0.09983595\n",
      "2019-06-18 22:28:09,502 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:28:09,504 EPOCH 63 done: loss 0.0997 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 22:29:35,387 TEST : loss 0.451578825712204 - score 0.714\n",
      "2019-06-18 22:29:40,704 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:29:42,363 epoch 64 - iter 0/402 - loss 0.28270745\n",
      "2019-06-18 22:30:14,260 epoch 64 - iter 40/402 - loss 0.10844250\n",
      "2019-06-18 22:30:43,554 epoch 64 - iter 80/402 - loss 0.10319679\n",
      "2019-06-18 22:31:16,248 epoch 64 - iter 120/402 - loss 0.10046672\n",
      "2019-06-18 22:31:45,129 epoch 64 - iter 160/402 - loss 0.09889107\n",
      "2019-06-18 22:32:14,608 epoch 64 - iter 200/402 - loss 0.09547224\n",
      "2019-06-18 22:32:42,044 epoch 64 - iter 240/402 - loss 0.09877959\n",
      "2019-06-18 22:33:13,670 epoch 64 - iter 280/402 - loss 0.09609296\n",
      "2019-06-18 22:33:40,183 epoch 64 - iter 320/402 - loss 0.09754537\n",
      "2019-06-18 22:34:07,033 epoch 64 - iter 360/402 - loss 0.09584768\n",
      "2019-06-18 22:34:36,869 epoch 64 - iter 400/402 - loss 0.09494162\n",
      "2019-06-18 22:34:38,405 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:34:38,406 EPOCH 64 done: loss 0.0949 - lr 0.0500 - bad epochs 0\n",
      "2019-06-18 22:36:04,302 TEST : loss 0.42919546365737915 - score 0.7342\n",
      "2019-06-18 22:36:09,617 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:36:11,316 epoch 65 - iter 0/402 - loss 0.00797445\n",
      "2019-06-18 22:36:40,966 epoch 65 - iter 40/402 - loss 0.08393785\n",
      "2019-06-18 22:37:14,531 epoch 65 - iter 80/402 - loss 0.08903891\n",
      "2019-06-18 22:37:42,346 epoch 65 - iter 120/402 - loss 0.09942603\n",
      "2019-06-18 22:38:13,004 epoch 65 - iter 160/402 - loss 0.09552945\n",
      "2019-06-18 22:38:46,816 epoch 65 - iter 200/402 - loss 0.10094180\n",
      "2019-06-18 22:39:13,778 epoch 65 - iter 240/402 - loss 0.10292280\n",
      "2019-06-18 22:39:41,365 epoch 65 - iter 280/402 - loss 0.10270831\n",
      "2019-06-18 22:39:50,304 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:39:50,309 Exiting from training early.\n",
      "2019-06-18 22:39:50,313 Saving model ...\n",
      "2019-06-18 22:39:55,618 Done.\n",
      "2019-06-18 22:39:55,625 ----------------------------------------------------------------------------------------------------\n",
      "2019-06-18 22:39:55,633 Testing using best model ...\n",
      "2019-06-18 22:41:22,072 0.7488\t0.7171\t0.7326\n",
      "2019-06-18 22:41:22,073 \n",
      "MICRO_AVG: acc 0.578 - f1-score 0.7326\n",
      "MACRO_AVG: acc 0.5187 - f1-score 0.6392444444444445\n",
      "-          tp: 0 - fp: 2 - fn: 35 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "College_Name tp: 104 - fp: 33 - fn: 37 - tn: 104 - precision: 0.7591 - recall: 0.7376 - accuracy: 0.5977 - f1-score: 0.7482\n",
      "Companies_worked_at tp: 181 - fp: 58 - fn: 82 - tn: 181 - precision: 0.7573 - recall: 0.6882 - accuracy: 0.5639 - f1-score: 0.7211\n",
      "L-College_Name tp: 105 - fp: 28 - fn: 27 - tn: 105 - precision: 0.7895 - recall: 0.7955 - accuracy: 0.6562 - f1-score: 0.7925\n",
      "L-Companies_worked_at tp: 189 - fp: 51 - fn: 71 - tn: 189 - precision: 0.7875 - recall: 0.7269 - accuracy: 0.6077 - f1-score: 0.7560\n",
      "L-Name     tp: 79 - fp: 8 - fn: 7 - tn: 79 - precision: 0.9080 - recall: 0.9186 - accuracy: 0.8404 - f1-score: 0.9133\n",
      "Name       tp: 77 - fp: 12 - fn: 9 - tn: 77 - precision: 0.8652 - recall: 0.8953 - accuracy: 0.7857 - f1-score: 0.8800\n",
      "U-College_Name tp: 1 - fp: 1 - fn: 1 - tn: 1 - precision: 0.5000 - recall: 0.5000 - accuracy: 0.3333 - f1-score: 0.5000\n",
      "U-Companies_worked_at tp: 42 - fp: 68 - fn: 38 - tn: 42 - precision: 0.3818 - recall: 0.5250 - accuracy: 0.2838 - f1-score: 0.4421\n",
      "2019-06-18 22:41:22,076 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dev_loss_history': [],\n",
       " 'dev_score_history': [],\n",
       " 'test_score': 0.7326,\n",
       " 'train_loss_history': [0.19494456864559828,\n",
       "  0.1939115002009999,\n",
       "  0.19734645368003134,\n",
       "  0.18257623558762062,\n",
       "  0.16943227295851826,\n",
       "  0.18599699165171651,\n",
       "  0.18081205983215304,\n",
       "  0.17127067643908125,\n",
       "  0.1746583947198308,\n",
       "  0.15683405879718154,\n",
       "  0.1502690320806717,\n",
       "  0.13643562435102996,\n",
       "  0.14545134799693948,\n",
       "  0.14352850059964764,\n",
       "  0.13466501158120028,\n",
       "  0.13289340272249273,\n",
       "  0.1292913343516452,\n",
       "  0.12182469621523102,\n",
       "  0.12411503006347377,\n",
       "  0.12359519671667274,\n",
       "  0.12156489881367158,\n",
       "  0.12187670634605398,\n",
       "  0.1163323722217656,\n",
       "  0.12312254132888284,\n",
       "  0.11414515104756426,\n",
       "  0.11460021330942562,\n",
       "  0.11136574598390665,\n",
       "  0.11380362604363631,\n",
       "  0.11198965602772153,\n",
       "  0.10533681278353307,\n",
       "  0.10926857564728058,\n",
       "  0.10718384726130548,\n",
       "  0.10442720870695897,\n",
       "  0.09965691947614524,\n",
       "  0.09491823016855847]}"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = 'resources/taggers/flair-wordflairembeddings'\n",
    "if downsample < 1.0:\n",
    "  model_name += '-test'\n",
    "\n",
    "use_checkpoints = downsample == 1.0\n",
    "checkpoint_path = Path(model_name) / 'checkpoint.pt'\n",
    "if use_checkpoints and checkpoint_path.exists():\n",
    "    checkpoint = tagger.load_checkpoint(checkpoint_path)\n",
    "    trainer: ModelTrainer = ModelTrainer.load_from_checkpoint(checkpoint, corpus)\n",
    "else:\n",
    "    trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# start/continue training\n",
    "trainer.train(\n",
    "    model_name, learning_rate=0.1, mini_batch_size=32,\n",
    "    #anneal_with_restarts=True,\n",
    "    max_epochs=50,\n",
    "    train_with_dev=False,  # notebook output shows a run with train_with_dev=True\n",
    "    checkpoint=use_checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSZT4z6K5C0U"
   },
   "outputs": [],
   "source": [
    "# only works when training the model in one go,\n",
    "# otherwise the loss.tsv file gets overwritten by flair\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_training_curves(model_name + '/loss.tsv')\n",
    "plotter.plot_weights(model_name + '/weights.txt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "flair_nlp_colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
