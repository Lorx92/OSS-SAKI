{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Resume NER\n",
    "## Extract Information from Resumes using NER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - NER with Spacy\n",
    "We will be using the preprocessed data from part one to start training NER models with spacy (https://spacy.io/) and to perform some additional preprocessing on our data before moving to training with flair. \n",
    "We will also explore evaluation metrics for NER, and decide how we want to quantify the performance of our trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690 resumes loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "path = \"converted_resumes.json\"\n",
    "with open(path, 'rt') as input_file:\n",
    "    resumes = json.load(input_file)\n",
    "print('{} resumes loaded'.format(len(resumes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare Training Data for NER model training\n",
    "We need to do some more preprocessing of our training data before we can train our model.\n",
    "\n",
    "As a first step, we will gather all resumes that contain at least one training annotation for each of the entities chosen in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathered 466 training examples\n"
     ]
    }
   ],
   "source": [
    "chosen_entity_labels = ['Name', 'College Name', 'Companies worked at']\n",
    "\n",
    "# gathers all resumes which have all of the chosen entities above\n",
    "def gather_candidates(dataset,entity_labels):\n",
    "    candidates = list()\n",
    "    for resume in dataset:\n",
    "        res_ent_labels = list(zip(*resume[1][\"entities\"]))[2]\n",
    "        if set(entity_labels).issubset(res_ent_labels):\n",
    "            candidates.append(resume)\n",
    "    return candidates\n",
    "\n",
    "training_data = gather_candidates(resumes, chosen_entity_labels)\n",
    "print(\"Gathered {} training examples\".format(len(training_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have those training examples which contain the entities we are interested in.\n",
    "\n",
    "The next step is to remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466 total resumes as training data, reducing to 345 unique\n"
     ]
    }
   ],
   "source": [
    "# look for text duplicates in our training data\n",
    "# if duplicates found: only use the first resume encountered\n",
    "import collections\n",
    "text_to_res_index = collections.OrderedDict()\n",
    "for index, res in enumerate(training_data):\n",
    "    text = res[0]\n",
    "    if text not in text_to_res_index:\n",
    "        text_to_res_index[text] = []\n",
    "    text_to_res_index[text].append(index)\n",
    "duplicates_indices = []\n",
    "unique_resumes_indices = []\n",
    "for _, indices in text_to_res_index.items():\n",
    "    if len(indices) > 1:\n",
    "        duplicates_indices.extend(indices)\n",
    "        unique_resumes_indices.append(indices[0])\n",
    "    else:\n",
    "        unique_resumes_indices.extend(indices)\n",
    "print('{} total resumes as training data, reducing to {} unique'.format(\n",
    "        len(training_data), len(unique_resumes_indices)))\n",
    "print_duplicates = False\n",
    "if print_duplicates:\n",
    "    for index in duplicates_indices:\n",
    "        res = resumes[index]\n",
    "        text = res[0]\n",
    "        ents = res[1]['entities']\n",
    "        print('{: >4}: {}...'.format(index, text[:80].replace('\\n', r'\\n')))\n",
    "        for ent in ents:\n",
    "            print('      {} = {}'.format(ent[2], text[ent[0]:ent[1]].replace('\\n', r'\\n')))\n",
    "\n",
    "# update training_data to only use unique resumes\n",
    "training_data = [training_data[i] for i in unique_resumes_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove other entity annotations from training data\n",
    "Now that we have our training data, we want to remove all but relevant (chosen) entity annotations from this data, so that the model we train will only train for our entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(X) = 345\n"
     ]
    }
   ],
   "source": [
    "# filter all annotation based on filter list\n",
    "def filter_ents(ents, filter):\n",
    "    filtered = [ent for ent in ents if ent[2] in filter]\n",
    "    return filtered\n",
    "\n",
    "# remove all but relevant (chosen) entity annotations\n",
    "X = [\n",
    "    [resume[0], {'entities': \n",
    "        filter_ents(resume[1]['entities'], chosen_entity_labels)\n",
    "    }]\n",
    "    for resume in training_data\n",
    "]\n",
    "print('len(X) = {}'.format(len(X)))\n",
    "#print(X[5][1]['entities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove resumes that cause errors in spacy\n",
    "Some of the resumes might cause errors in spacy. We don't need to get into details as to why, suffice to say it has to do with whitespace and syntax in the entity annotations. If these resumes are not removed from our training data, spacy will throw an exception during training, so we need to remove them first. \n",
    "\n",
    "We will use the remove_bad_data function below to do this. This function does the following:\n",
    "* calls train_spacy_ner with debug=True and n_iter=1. This causes spacy to process the documents one-by-one, and gather the documents that throw an exception in a list of \"bad docs\" which it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Exception thrown when processing doc:\n",
      "(\"Neeraj Dwivedi\\nSenior Sales Executive - Kansai Nerolac Paints Ltd\\n\\nMumbai, Maharashtra - Email me on Indeed: indeed.com/r/Neeraj-Dwivedi/8f053ed44cdef8b2\\n\\nWORK EXPERIENCE\\n\\nSenior Sales Executive\\n\\nKansai Nerolac Paints Ltd -  Mumbai, Maharashtra -\\n\\nAugust 2017 to Present\\n\\nIncreased the sales from -22% to +6% through existing dealers and prospecting new dealers to\\nachieve the target within 6 months of joining.\\n* Maintaining excellent relations with dealers to increase revenue by 10%.\\n* Management of sales team (up to 3 members)\\n* Achieved monthly targets amounting to 40 - 60 lakhs monthly with timely collection.\\n* Maintain accounts clarity with dealers and ensure that the credit notes reach the dealers on\\ntime.\\n* Business development by conducting regular meetings with influencers such as architects,\\nbuilding contractors, housing societies, painters.\\n* Ensuring proper visibility of the products at various counters while also managing field\\ndevelopment using new sign boards and painter meets.\\n* Provide Briefing to the dealers about the new schemes and target products.\\n* Handling depot and ensuring the proper service and support to the market and at the same\\ntime handling the depot team of 14-15 members.\\n* Handled the entire western and south market of Mumbai ( From Churchgate to Dahanu)\\n* Presently handling two territories Vasai and Virar ( Mira Road To Dahanu)\\n* Developed the Bhayandar and Mira Road market which was almost dead for the company.\\n\\nSenior Sales Executive\\n\\nGreenPly Industries Ltd -  Mumbai, Maharashtra -\\n\\nAugust 2015 to August 2017\\n\\nBuilding the business within the territory using a variety of sales techniques.\\n* Targeting potential dealers and distributors and assessing opportunities for sales.\\n* Arranging potential dealers meetings and selling products offering.\\n* Team handling and Achieving Primary and secondary sales figures as well individually and along\\nwith team.\\n* Create and execute a territory sales and channel development plan that meets or exceeds\\nestablished sales quotas and supports Company revenue and profit targets.\\n* Complete sales activity reports and presentations in a timely manner. Appointed 150 dealers\\nand 3 distributors for wall-coverings imported by Greenply.\\n\\nArea Sales Manager\\n\\nGloob Decor Interior Design Pvt. Ltd -  Pune, Maharashtra -\\n\\nhttps://www.indeed.com/r/Neeraj-Dwivedi/8f053ed44cdef8b2?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nJuly 2014 to July 2015\\n\\nMumbai, Rest of Maharashtra (Pune, Nashik, Kolhapur and more), Goa, Jaipur\\nArea Sales Manager (July 2014 - July 2015)\\n\\n* Planning and scheduling individual/ team assignments to achieve the pre set goals within\\ntime, quality and cost parameters. Formulating long term/short term strategic plans to enhance\\noperations.\\n* Tracking market/ competitor trends to keep a track regarding changing client's requirement/\\nexpectations.\\n* Dealer/ distributor channel visiting, handling channel sales team (up to 6 members) across\\nMaharashtra, Mumbai and Goa.\\n* Business development and Team Management.\\n* Developed the market for Pune, Kolhapur, Jaipur and appointed 2 franchisee stores and 3\\ndistributors (35-40 lakhs investment)\\n\\nInternship\\n\\nMumbai, Maharashtra -\\n\\nFebruary 2014 to May 2014\\n\\nProvided career counseling and vocational training to the students.\\n\\nProject Management\\n\\nAARK alliance -  Delhi, Delhi -\\n\\nMay 2013 to July 2013\\n\\nProject: Sales and Customer service of construction equipments\\n\\nEDUCATION\\n\\nMMS\\n\\nMaratha Mandir Babasaheb Gawde Institute of Management studies, mumbai\\n\\nSKILLS\\n\\nSALES (4 years), SALES AND (2 years), TEAM MANAGEMENT (1 year), AND MARKETING (Less\\nthan 1 year), BUSINESS MANAGEMENT (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nCore Skills:\\n\\n* 4 years of experience in corporate sales and marketing.\\n* Planing, strategizing and implementing business growth, requirement analysis and technical\\nguidance for clients\\n* Interpersonal skills, business management, negotiation skills, team management, lead\\ngeneration, loyalty programs management\\n\\n\\n\\nSoft Skills:\\n\\n* SAP, CRM, MS word, MS excel, MS power point, Tally, ERP, Social Media.\",) ({'entities': [[4016, 4017, 'Name'], [4015, 4016, 'Name'], [4002, 4003, 'Name'], [4001, 4002, 'Name'], [4000, 4001, 'Name'], [3999, 4000, 'Name'], [3959, 3960, 'Name'], [3872, 3873, 'Name'], [3851, 3852, 'Name'], [3758, 3759, 'Name'], [3700, 3701, 'Name'], [3699, 3700, 'Name'], [3686, 3687, 'Name'], [3685, 3686, 'Name'], [3662, 3663, 'Name'], [3661, 3662, 'Name'], [3608, 3609, 'Name'], [3524, 3525, 'Name'], [3523, 3524, 'Name'], [3516, 3517, 'Name'], [3515, 3516, 'Name'], [3445, 3507, 'College Name'], [3444, 3445, 'Name'], [3443, 3444, 'Name'], [3439, 3440, 'Name'], [3438, 3439, 'Name'], [3428, 3429, 'Name'], [3427, 3428, 'Name'], [3364, 3365, 'Name'], [3363, 3364, 'Name'], [3341, 3342, 'Name'], [3340, 3341, 'Name'], [3308, 3309, 'Name'], [3307, 3308, 'Name'], [3288, 3289, 'Name'], [3287, 3288, 'Name'], [3219, 3220, 'Name'], [3218, 3219, 'Name'], [3192, 3193, 'Name'], [3191, 3192, 'Name'], [3169, 3170, 'Name'], [3168, 3169, 'Name'], [3157, 3158, 'Name'], [3156, 3157, 'Name'], [3118, 3119, 'Name'], [3028, 3029, 'Name'], [2984, 2985, 'Name'], [2955, 2956, 'Name'], [2862, 2863, 'Name'], [2848, 2849, 'Name'], [2754, 2755, 'Name'], [2742, 2743, 'Name'], [2647, 2648, 'Name'], [2556, 2557, 'Name'], [2555, 2556, 'Name'], [2512, 2513, 'Name'], [2437, 2438, 'Name'], [2436, 2437, 'Name'], [2413, 2414, 'Name'], [2412, 2413, 'Name'], [2411, 2412, 'Name'], [2313, 2314, 'Name'], [2312, 2313, 'Name'], [2253, 2289, 'Companies worked at'], [2252, 2253, 'Name'], [2251, 2252, 'Name'], [2232, 2233, 'Name'], [2231, 2232, 'Name'], [2171, 2172, 'Name'], [2077, 2078, 'Name'], [2003, 2004, 'Name'], [1913, 1914, 'Name'], [1902, 1903, 'Name'], [1805, 1806, 'Name'], [1735, 1736, 'Name'], [1649, 1650, 'Name'], [1569, 1570, 'Name'], [1568, 1569, 'Name'], [1541, 1542, 'Name'], [1540, 1541, 'Name'], [1492, 1515, 'Companies worked at'], [1491, 1492, 'Name'], [1490, 1491, 'Name'], [1467, 1468, 'Name'], [1466, 1467, 'Name'], [1380, 1381, 'Name'], [1304, 1305, 'Name'], [1219, 1220, 'Name'], [1172, 1173, 'Name'], [1081, 1082, 'Name'], [1004, 1005, 'Name'], [951, 952, 'Name'], [860, 861, 'Name'], [809, 810, 'Name'], [718, 719, 'Name'], [712, 713, 'Name'], [617, 618, 'Name'], [531, 532, 'Name'], [486, 487, 'Name'], [411, 412, 'Name'], [364, 365, 'Name'], [271, 272, 'Name'], [270, 271, 'Name'], [247, 248, 'Name'], [246, 247, 'Name'], [196, 221, 'Companies worked at'], [195, 196, 'Name'], [194, 195, 'Name'], [171, 172, 'Name'], [170, 171, 'Name'], [154, 155, 'Name'], [153, 154, 'Name'], [66, 67, 'Name'], [65, 66, 'Name'], [40, 65, 'Companies worked at'], [40, 65, 'Companies worked at'], [14, 15, 'Name'], [0, 14, 'Name']]},)\n",
      "Exception thrown when processing doc:\n",
      "(\"Alok Gond\\nArea Business Manager - Zuventus Healthcare Pvt Ltd\\n\\nMumbai, Maharashtra - Email me on Indeed: indeed.com/r/Alok-Gond/6e691dc668a54602\\n\\nTo work in an organization where I am able to contribute to the organization's growth and\\nprofitability with my skill and in turn get an opportunity to gain exposure and expertise that would\\nhelp me build a strong and successful career.\\n\\nWilling to relocate to: Mumbai, Maharashtra\\n\\nWORK EXPERIENCE\\n\\nArea Business Manager\\n\\nZuventus Healthcare Pvt Ltd -\\n\\nJuly 2012 to Present\\n\\nworking in super speciality division of gastro and respiratory and handling team of 5 medical\\nrepresentative.\\n\\nZuventus Healthcare Pvt Ltd -\\n\\nNovember 2008 to Present\\n\\n2008.\\n\\nDescription: Working in speciality Division of Gastro & Respiratory\\n\\npharmaceuticals sales\\n\\nNicholas Piramal Pvt. Ltd -\\n\\nAugust 2007 to November 2008\\n\\nDescription: Worked in speciality Division of Cardio & Diabetic.\\n\\nBlack&White, VAT -\\n\\nJuly 2006 to August 2007\\n\\n69, J&B, Red Label.\\n\\n2): Worked in SUN Pharmaceuticals from July 2006 to Aug 2007.\\n\\nDescription: Worked in speciality Division of Orthopedics,\\nRhumatology & Dermatology. .\\n\\nEDUCATION\\n\\nbsc botany in Botany\\n\\nhttps://www.indeed.com/r/Alok-Gond/6e691dc668a54602?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nMumbai University -  Mumbai, Maharashtra\\n\\nMarch 2006\\n\\nS.S.C in 1st\\n\\nGlobe Mill Passage High School\\n\\nMarch 2001\\n\\nSKILLS\\n\\npharmaceuticals, Sales\\n\\nADDITIONAL INFORMATION\\n\\n* Optimistic attitude and Self Confidence.\\n* Good interpersonal skills\\n* Command over my language\\n* Self motivated, Very hard working\",) ({'entities': [[1526, 1527, 'College Name'], [1499, 1500, 'College Name'], [1471, 1472, 'College Name'], [1428, 1429, 'College Name'], [1427, 1428, 'College Name'], [1404, 1405, 'College Name'], [1403, 1404, 'College Name'], [1380, 1381, 'College Name'], [1379, 1380, 'College Name'], [1372, 1373, 'College Name'], [1371, 1372, 'College Name'], [1360, 1361, 'College Name'], [1359, 1360, 'College Name'], [1328, 1329, 'College Name'], [1327, 1328, 'College Name'], [1314, 1315, 'College Name'], [1313, 1314, 'College Name'], [1302, 1303, 'College Name'], [1301, 1302, 'College Name'], [1261, 1278, 'College Name'], [1260, 1261, 'College Name'], [1259, 1260, 'College Name'], [1258, 1259, 'College Name'], [1165, 1166, 'College Name'], [1164, 1165, 'College Name'], [1143, 1144, 'College Name'], [1142, 1143, 'College Name'], [1132, 1133, 'College Name'], [1131, 1132, 'College Name'], [1102, 1103, 'College Name'], [1043, 1044, 'College Name'], [1042, 1043, 'College Name'], [995, 1014, 'Companies worked at'], [980, 981, 'College Name'], [979, 980, 'College Name'], [959, 960, 'College Name'], [958, 959, 'College Name'], [933, 934, 'College Name'], [932, 933, 'College Name'], [914, 925, 'Companies worked at'], [913, 914, 'College Name'], [912, 913, 'College Name'], [847, 848, 'College Name'], [846, 847, 'College Name'], [817, 818, 'College Name'], [816, 817, 'College Name'], [789, 814, 'Companies worked at'], [788, 789, 'College Name'], [787, 788, 'College Name'], [765, 766, 'College Name'], [764, 765, 'College Name'], [696, 697, 'College Name'], [695, 696, 'College Name'], [689, 690, 'College Name'], [688, 689, 'College Name'], [663, 664, 'College Name'], [662, 663, 'College Name'], [633, 660, 'Companies worked at'], [632, 633, 'College Name'], [631, 632, 'College Name'], [615, 616, 'College Name'], [521, 522, 'College Name'], [520, 521, 'College Name'], [499, 500, 'College Name'], [498, 499, 'College Name'], [469, 496, 'Companies worked at'], [468, 469, 'College Name'], [467, 468, 'College Name'], [445, 446, 'College Name'], [444, 445, 'College Name'], [428, 429, 'College Name'], [427, 428, 'College Name'], [383, 384, 'College Name'], [382, 383, 'College Name'], [336, 337, 'College Name'], [235, 236, 'College Name'], [145, 146, 'College Name'], [144, 145, 'College Name'], [62, 63, 'College Name'], [61, 62, 'College Name'], [34, 61, 'Companies worked at'], [9, 10, 'College Name'], [0, 9, 'Name']]},)\n",
      "Exception thrown when processing doc:\n",
      "(\"Punit Raghav\\nSales Manager - Mukund Overseas - Magnum\\n\\nThane, Maharashtra - Email me on Indeed: indeed.com/r/Punit-Raghav/f36e9e4d0857ac5b\\n\\nA competent professional with over 8 years of experience in:\\n\\n- Handling Dealer & Distributor - Business Development - Handling Projects\\n- Architect & Interior Designer - Handling Carpenters & Contractors\\n\\nCore Functional Skills:\\n\\n* Effectively meet deadlines, achieve targets and work under pressure.\\n* Company success driven - passionate about company's product line.\\n* Accounting-related computer literacy.\\n* Supervising the performance of dealers / distributors with key emphasis on achieving revenue\\ntargets.\\n* Excellent communication skills, written and verbal.\\n* Effective presentation of complex issues.\\n* High level of negotiation skills.\\n\\nWilling to relocate to: Maharashtra - South india\\n\\nWORK EXPERIENCE\\n\\nSales Manager\\n\\nMukund Overseas - Magnum -  Mumbai, Maharashtra -\\n\\nAugust 2007 to Present\\n\\nPune, Nashik, Kolhapur, Satara, Solapur, Nagpur, Aurangabad.\\nAndhra Pradesh - Hydrabad, Vijaywada, Guntur, Kakinada, Rajmandri, Vishakapattnam.\\nKarnataka - Belgaum, Hubli, Dhavangiri, Gulbarga, Shimoga, Chikmanglaur, Hassan, Manglore,\\nBanglore, Mandya, Mysore,\\nKerala - Thallseri, Calicut, Trissur, Ernakulam, Kolam, Trivendrapuram, Kannur, Changnacheri.\\nTamil Nadu - Chennai, Selam, Erode, Trichypalli, Trivenvelli, Madurai, Coimbatore.\\nRajasthan - Udaipur, Jodhpur, Ajmer, Jaipur.\\nMadhya Pradesh - Bhopal, Indore, Gwalior, Jabalpur.\\n\\nHighlight of Activities:\\n* Managed the sales operation of the firm to accomplish business strategy goals.\\n* Advised senior management on best routes and strategies to implement in order to achieve\\nbusiness development.\\n* Customer relationship Management:\\n* Sustained sound relationship with existing clientele - Quantified client requirements through\\nclose contact.\\n* Kept in close contact with clientele to identify new opportunities and customers - Maintained\\navailability for addressing customer issues, queries and requirements.\\n* Regularly updated contact database.\\n\\nhttps://www.indeed.com/r/Punit-Raghav/f36e9e4d0857ac5b?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nNotable Initiatives:\\n* Motivated and organized relevant market data which was shared by sales staff.\\n* Ensured internal communication was efficient throughout company; sales, marketing and\\ncustomer support.\\n* Tracked sales (using computer or spreadsheets) to provide accurate reports.\\n* Participated in conferences, group meetings, trade shows and exhibitions to deliver\\npresentations on customer sites - Demonstrated new products/services at various sites used by\\ncompany's clients.\\n* Monitored competitor activities closely to identify any business threats.\\n\\nSales marketing\\n\\nEDUCATION\\n\\nB.Com\\n\\nUniversity of Mumbai -  Mumbai, Maharashtra\",) ({'entities': [[2749, 2769, 'College Name'], [2748, 2749, 'College Name'], [2747, 2748, 'College Name'], [2741, 2742, 'College Name'], [2740, 2741, 'College Name'], [2730, 2731, 'College Name'], [2729, 2730, 'College Name'], [2713, 2714, 'College Name'], [2712, 2713, 'College Name'], [2636, 2637, 'College Name'], [2617, 2618, 'College Name'], [2523, 2524, 'College Name'], [2437, 2438, 'College Name'], [2359, 2360, 'College Name'], [2341, 2342, 'College Name'], [2253, 2254, 'College Name'], [2173, 2174, 'College Name'], [2152, 2153, 'College Name'], [2151, 2152, 'College Name'], [2150, 2151, 'College Name'], [2054, 2055, 'College Name'], [2053, 2054, 'College Name'], [2015, 2016, 'College Name'], [1944, 1945, 'College Name'], [1848, 1849, 'College Name'], [1833, 1834, 'College Name'], [1737, 1738, 'College Name'], [1701, 1702, 'College Name'], [1679, 1680, 'College Name'], [1588, 1589, 'College Name'], [1507, 1508, 'College Name'], [1482, 1483, 'College Name'], [1481, 1482, 'College Name'], [1429, 1430, 'College Name'], [1384, 1385, 'College Name'], [1301, 1302, 'College Name'], [1207, 1208, 'College Name'], [1181, 1182, 'College Name'], [1090, 1091, 'College Name'], [1007, 1008, 'College Name'], [946, 947, 'College Name'], [945, 946, 'College Name'], [922, 923, 'College Name'], [921, 922, 'College Name'], [872, 887, 'Companies worked at'], [871, 872, 'College Name'], [870, 871, 'College Name'], [856, 857, 'College Name'], [855, 856, 'College Name'], [839, 840, 'College Name'], [838, 839, 'College Name'], [788, 789, 'College Name'], [787, 788, 'College Name'], [751, 752, 'College Name'], [707, 708, 'College Name'], [653, 654, 'College Name'], [644, 645, 'College Name'], [549, 550, 'College Name'], [509, 510, 'College Name'], [441, 442, 'College Name'], [370, 371, 'College Name'], [369, 370, 'College Name'], [345, 346, 'College Name'], [344, 345, 'College Name'], [276, 277, 'College Name'], [201, 202, 'College Name'], [200, 201, 'College Name'], [139, 140, 'College Name'], [138, 139, 'College Name'], [54, 55, 'College Name'], [53, 54, 'College Name'], [29, 44, 'Companies worked at'], [12, 13, 'College Name'], [0, 12, 'Name']]},)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception thrown when processing doc:\n",
      "(\"Sheldon Creado\\nSr. Manager - Regional Sales\\n\\nMumbai, Maharashtra - Email me on Indeed: indeed.com/r/Sheldon-Creado/\\nb73c053d2691e84a\\n\\n* Result-oriented professional with experience of 15 years in Sales Planning/ Execution, Process\\nImprovement and Business Development.\\n* Excellent track record in performing challenging strategic & leadership roles, building\\nstrategic service plans and CSAT.\\n* Demonstrated effectiveness in high-profile executive roles driving large scale gains in business\\nvolumes through on-ground business strategies and consistent acquisition, deepening &\\nretention of customer base.\\n\\nWilling to relocate to: Mumbai, Maharashtra - Pune, Maharashtra - Bangalore, Karnataka\\n\\nWORK EXPERIENCE\\n\\nSr. Manager - Regional Sales\\n\\nTata Teleservices Ltd\\n\\nJob Profile:\\n* Managed and developed an assigned portfolio of accounts, increasing product penetration and\\nrevenue market share.\\n* Successfully achieved set Business Acquisition, Revenue Maximization & Retention targets\\nas per Annual Operating Plan through efficient account management.\\n* Lead and contributed to negotiation of business terms and arrangements. Devised Customized\\nSolutions for clients, working collectively with Internal Pre-Sales teams.\\n* Ensured revenue targets are met through Micro-Monitoring Account Performance and pipeline\\nactivities as per planned market strategy.\\n* Developed competitive sales approach-strategy considering all significant factors for the\\naccounts assigned to me. Prospected potential customers, analyzed their requirements and\\nconverted them to Enterprise based solutions.\\n* Augmented market penetration by identifying and appointing Business partners keeping in\\nsync with the company's Go-To Market strategy.\\n* Imparted Product Training to business partners and ensured their development through regular\\nrefresher courses/programs on various innovative telecom services/solutions offered by the\\ncompany.\\n* Ensured adherence to system, procedures and Market Outstanding norms together with teams\\nin Project Delivery, Service & Revenue Assurance.\\n* Orchestrated Tech-Check programs at high revenue accounts, documented their feedback and\\nensured concern areas were addressed and issues resolved to the customers satisfaction.\\n* Coordinated with all internal teams of purchase, operations and business partners to deliver\\nservices within agreed TAT.\\n* Drove and Motivated the team towards achieving set business objectives emphasizing on the\\nbenefits of Efficient Account management.\\n* Products included: Corporate Voice Solutions, Enterprise Data, IOT & Data Centre services.\\n\\nhttps://www.indeed.com/r/Sheldon-Creado/b73c053d2691e84a?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Sheldon-Creado/b73c053d2691e84a?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nSr. Manager - Enterprise Sales\\n\\nReliance Communications Ltd -\\n\\nApril 2011 to November 2015\\n\\nJob Profile:\\n* Developed enterprise business sales strategies and ensured achievement of projected sales\\nnumbers through effective implementation of business plans & projects in line with\\ncorporate strategy.\\n* Acquired business SME & large accounts through Rapport Building with key stake holders.\\n* Prepared, presented and successfully negotiated proposals with Enterprise customers.\\n* Accountable for the delivery of the Enterprise Account revenue targets.\\n* Built a liaison with Internal teams of Product Marketing, Business Solutions & Service\\nDelivery, in ensuring commissioning of services within industry norms.\\n* Ensured high CSAT scores through close monitoring of customer feedback and resolution of\\nqueries within agreed timelines.\\n* Researched information on competitive pricing, market activities, and other information about\\ntargeted markets.\\n* Product Portfolio included Wireless & Enterprise Solutions including Data and IDC services.\\n\\nAssistant Manager - Sales\\n\\nBharti Airtel Ltd -\\n\\nMay 2007 to April 2011\\n\\nJob Profile:\\n* Effectively managed a team of Sales Account Managers involving them in Account\\nManagement, Sales, Revenue and Retention activities.\\n* Augmented product penetration & revenue market share in assigned corporate accounts\\nthrough Cross selling and Upselling of services.\\n* Corporate Partner Management: Ensured a larger market footprint in the corporate market\\nthrough appointment of proficient business partners, their training and enablement.\\n* Product portfolio included Enterprise Voice and Data services.\\n\\nAssistant Manager (Sales)\\n\\nBharti Airtel Ltd -\\n\\nNovember 2005 to April 2011\\n\\nTerritory Manager\\n\\nBharti Airtel Ltd -\\n\\nNovember 2005 to April 2007\\n\\nJob Profile:\\n* Effectively managed and achieved sales numbers through effective Account Management\\n& Marketing activities.\\n* Lead a team of sales managers, driving them towards achieving set goals and surpassing set\\ntargets.\\n\\n\\n\\n* Product Portfolio included: Mobile services, Blackberry Enterprise Solutions & Wireless USB\\nData Cards.\\n\\nAssistant Manager (Sales)\\n\\nGodfrey Phillips India Ltd -\\n\\nSeptember 2003 to November 2005\\n\\n* Developed New Corporate Accounts to augment market penetration.\\n* Brand Management & Marketing - Ensured customer loyalty through Product Innovation\\n& Effective Marketing Ploys.\\n* Ensured CSAT through effective account management.\\n\\nTerritory Sales in Charge\\n\\nParle Products Pvt. Ltd -\\n\\nApril 2002 to September 2003\\n\\nSuccessfully lead a Team of 3 channel partners and their respective Sales Teams.\\n* Achieved sales and revenue targets as pre-defined by the company through effective customer\\nand channel partner management.\\n* Ensured Viability of Channel Partners by driving them to achieve and surpass required sales\\nnumbers.\\n\\nSales Executive\\n\\nCadbury India Ltd -\\n\\nSeptember 2000 to April 2002\\n\\nJob Profile:\\n* Efficiently managed the development and expansion of business in particularly 'A' class retail\\nmarket segment.\\n* Lead a team of sales executives, towards ensuring objectives as set by the company are met.\\n\\nEDUCATION\\n\\nMBA\\n\\nNational Institute of Management -  Mumbai, Maharashtra\\n\\nApril 2009\\n\\nBachelor of Commerce in Reliance Communications Ltd\\n\\nMumbai University -  Mumbai, Maharashtra\\n\\nMarch 2000\\n\\n\\n\\nSKILLS\\n\\nIT Consulting, Business Development, Customer Service, Service Delivery, Liasoning, Technical\\nSales Presentations, Cross-functional Coordination, Coaching & Mentoring, Identifying\\nSales Opportunities, Product Knowledge, C-Level Relationships Level, High Analytical Skills,\\nConceptual Ability,\",) ({'entities': [[6384, 6385, 'College Name'], [6291, 6292, 'College Name'], [6205, 6206, 'College Name'], [6111, 6112, 'College Name'], [6110, 6111, 'College Name'], [6103, 6104, 'College Name'], [6102, 6103, 'College Name'], [6101, 6102, 'College Name'], [6100, 6101, 'College Name'], [6089, 6090, 'College Name'], [6088, 6089, 'College Name'], [6047, 6048, 'College Name'], [6046, 6047, 'College Name'], [5994, 5995, 'College Name'], [5993, 5994, 'College Name'], [5982, 5983, 'College Name'], [5981, 5982, 'College Name'], [5926, 5981, 'College Name'], [5925, 5926, 'College Name'], [5924, 5925, 'College Name'], [5920, 5921, 'College Name'], [5919, 5920, 'College Name'], [5909, 5910, 'College Name'], [5908, 5909, 'College Name'], [5814, 5815, 'College Name'], [5798, 5799, 'College Name'], [5701, 5702, 'College Name'], [5688, 5689, 'College Name'], [5687, 5688, 'College Name'], [5658, 5659, 'College Name'], [5657, 5658, 'College Name'], [5638, 5655, 'Companies worked at'], [5637, 5638, 'College Name'], [5636, 5637, 'College Name'], [5620, 5621, 'College Name'], [5619, 5620, 'College Name'], [5610, 5611, 'College Name'], [5516, 5517, 'College Name'], [5484, 5485, 'College Name'], [5390, 5391, 'College Name'], [5309, 5310, 'College Name'], [5308, 5309, 'College Name'], [5279, 5280, 'College Name'], [5278, 5279, 'College Name'], [5253, 5278, 'Companies worked at'], [5252, 5253, 'College Name'], [5251, 5252, 'College Name'], [5225, 5226, 'College Name'], [5224, 5225, 'College Name'], [5171, 5172, 'College Name'], [5142, 5143, 'College Name'], [5057, 5058, 'College Name'], [4991, 4992, 'College Name'], [4990, 4991, 'College Name'], [4958, 4959, 'College Name'], [4957, 4958, 'College Name'], [4929, 4957, 'Companies worked at'], [4928, 4929, 'College Name'], [4927, 4928, 'College Name'], [4901, 4902, 'College Name'], [4900, 4901, 'College Name'], [4888, 4889, 'College Name'], [4794, 4795, 'College Name'], [4793, 4794, 'College Name'], [4792, 4793, 'College Name'], [4791, 4792, 'College Name'], [4782, 4783, 'College Name'], [4689, 4690, 'College Name'], [4665, 4666, 'College Name'], [4579, 4580, 'College Name'], [4566, 4567, 'College Name'], [4565, 4566, 'College Name'], [4537, 4538, 'College Name'], [4536, 4537, 'College Name'], [4517, 4536, 'Companies worked at'], [4516, 4517, 'College Name'], [4515, 4516, 'College Name'], [4497, 4498, 'College Name'], [4496, 4497, 'College Name'], [4468, 4469, 'College Name'], [4467, 4468, 'College Name'], [4448, 4467, 'Companies worked at'], [4447, 4448, 'College Name'], [4446, 4447, 'College Name'], [4420, 4421, 'College Name'], [4419, 4420, 'College Name'], [4354, 4355, 'College Name'], [4270, 4271, 'College Name'], [4180, 4181, 'College Name'], [4131, 4132, 'College Name'], [4045, 4046, 'College Name'], [3992, 3993, 'College Name'], [3911, 3912, 'College Name'], [3898, 3899, 'College Name'], [3897, 3898, 'College Name'], [3874, 3875, 'College Name'], [3873, 3874, 'College Name'], [3854, 3873, 'Companies worked at'], [3853, 3854, 'College Name'], [3852, 3853, 'College Name'], [3826, 3827, 'College Name'], [3825, 3826, 'College Name'], [3731, 3732, 'College Name'], [3713, 3714, 'College Name'], [3617, 3618, 'College Name'], [3584, 3585, 'College Name'], [3493, 3494, 'College Name'], [3422, 3423, 'College Name'], [3333, 3334, 'College Name'], [3259, 3260, 'College Name'], [3172, 3173, 'College Name'], [3082, 3083, 'College Name'], [3062, 3063, 'College Name'], [2979, 2980, 'College Name'], [2887, 2888, 'College Name'], [2874, 2875, 'College Name'], [2873, 2874, 'College Name'], [2845, 2846, 'College Name'], [2844, 2845, 'College Name'], [2815, 2844, 'Companies worked at'], [2814, 2815, 'College Name'], [2813, 2814, 'College Name'], [2782, 2783, 'College Name'], [2781, 2782, 'College Name'], [2780, 2781, 'College Name'], [2682, 2683, 'College Name'], [2584, 2585, 'College Name'], [2583, 2584, 'College Name'], [2490, 2491, 'College Name'], [2448, 2449, 'College Name'], [2356, 2357, 'College Name'], [2328, 2329, 'College Name'], [2233, 2234, 'College Name'], [2145, 2146, 'College Name'], [2054, 2055, 'College Name'], [2004, 2005, 'College Name'], [1913, 1914, 'College Name'], [1904, 1905, 'College Name'], [1813, 1814, 'College Name'], [1718, 1719, 'College Name'], [1671, 1672, 'College Name'], [1581, 1582, 'College Name'], [1535, 1536, 'College Name'], [1446, 1447, 'College Name'], [1354, 1355, 'College Name'], [1311, 1312, 'College Name'], [1219, 1220, 'College Name'], [1144, 1145, 'College Name'], [1051, 1052, 'College Name'], [984, 985, 'College Name'], [893, 894, 'College Name'], [871, 872, 'College Name'], [777, 778, 'College Name'], [764, 765, 'College Name'], [763, 764, 'College Name'], [742, 763, 'Companies worked at'], [741, 742, 'College Name'], [740, 741, 'College Name'], [711, 712, 'College Name'], [710, 711, 'College Name'], [694, 695, 'College Name'], [693, 694, 'College Name'], [606, 607, 'College Name'], [605, 606, 'College Name'], [577, 578, 'College Name'], [491, 492, 'College Name'], [392, 393, 'College Name'], [358, 359, 'College Name'], [268, 269, 'College Name'], [230, 231, 'College Name'], [133, 134, 'College Name'], [132, 133, 'College Name'], [115, 116, 'College Name'], [44, 45, 'College Name'], [43, 44, 'College Name'], [14, 15, 'College Name'], [0, 14, 'Name']]},)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 33393.11133236157}\n",
      "Unfiltered training data size:  345\n",
      "Filtered training data size:  341\n",
      "Bad data size:  4\n"
     ]
    }
   ],
   "source": [
    "from spacy_train_resume_ner import train_spacy_ner\n",
    "\n",
    "def remove_bad_data(training_data):\n",
    "    model, baddocs = train_spacy_ner(training_data, debug=True, n_iter=1)\n",
    "    # training data is a list of lists with each list containing text and annotations\n",
    "    # baddocs is a set of strings/resume texts.\n",
    "    # filter bad docs and store filter result (good docs) in filtered variable\n",
    "    filtered = [data for data in training_data if data[0] not in baddocs]\n",
    "    print(\"Unfiltered training data size: \",len(training_data))\n",
    "    print(\"Filtered training data size: \", len(filtered))\n",
    "    print(\"Bad data size: \", len(baddocs))\n",
    "    return filtered\n",
    "\n",
    "X_filtered = remove_bad_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of bad docs? Size of new (filtered) training data? \n",
    "4 bad docs; 341"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Test Split\n",
    "Now before we train our model, we have to split our available training data into training and test sets. Splitting our data into train and test (or holdout) datasets is a fundamental technique in machine learning, and essential to avoid the problem of overfitting.\n",
    "\n",
    "relevant links:\n",
    "* https://machinelearningmastery.com/a-simple-intuition-for-overfitting/\n",
    "* https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "\n",
    "##### What is overfitting and how does doing a train/test split help us avoid overfitting when training our models?\n",
    "A model overfits when it performs significantly better on training data than on unknown data. Reserving parts of a dataset as the test set (must not overlap with the training set) for evaluation of the model allows to get an objective view of its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345 docs split into 256 for training and 85 for testing\n",
      "Name: support is 266 for training, 88 for testing\n",
      "College Name: support is 435 for training, 135 for testing\n",
      "Companies worked at: support is 1282 for training, 349 for testing\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def train_test_split(X, train_fraction):\n",
    "    train_size = round(len(X) * train_fraction)\n",
    "    shuffled_X = random.sample(X, k=len(X))\n",
    "    train = shuffled_X[:train_size]\n",
    "    test = shuffled_X[train_size:]\n",
    "    return train,test\n",
    "\n",
    "# populate train and test sets\n",
    "train, test = train_test_split(X_filtered, 0.75)\n",
    "assert len(train) + len(test) == len(X_filtered)\n",
    "print('{} docs split into {} for training and {} for testing'.format(\n",
    "    len(X), len(train), len(test)))\n",
    "\n",
    "# get entity counts for the test set\n",
    "def get_entity_count(dataset, entity_name):\n",
    "    count = 0\n",
    "    for res in dataset:\n",
    "        ents = res[1]['entities']\n",
    "        for ent in ents:\n",
    "            if ent[2] == entity_name:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "for entity_name in chosen_entity_labels:\n",
    "    print('{}: support is {} for training, {} for testing'.format(\n",
    "        entity_name,\n",
    "        get_entity_count(train, entity_name),\n",
    "        get_entity_count(test, entity_name)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train a spacy ner model with our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import spacy\n",
    "\n",
    "model_directory = \"custom_nlp\"\n",
    "if os.path.exists(model_directory):\n",
    "    custom_nlp = spacy.load(model_directory)\n",
    "else:\n",
    "    custom_nlp,_= train_spacy_ner(train,n_iter=20)\n",
    "    custom_nlp.to_disk(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspect NER predictions on one sample resume\n",
    "Now that we have a trained model, let's see how it works on one of our resumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetch one resume out of our test dataset and store to the \"resume\" variable\n",
    "resume = random.choice(test)\n",
    "## create a spacy doc out of the resume using our trained model and save to the \"doc\" variable \n",
    "doc = custom_nlp(resume[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will output the predicted entities and the existing annotated entities in that doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTED:\n",
      "Name = Puneet Bhandari\n",
      "Companies worked at = Microsoft IT\n",
      "Companies worked at = Microsoft IT\n",
      "Companies worked at = Microsoft\n",
      "Companies worked at = IIT Roorkee\n",
      "College Name = Shri Vaishnav Institute of Technology and Science, RGPV University\n",
      "\n",
      "LABELED:\n",
      "College Name = Shri Vaishnav Institute of Technology and Science, RGPV University\n",
      "College Name = IIT Roorkee\n",
      "Companies worked at = Microsoft\n",
      "Companies worked at = Microsoft\n",
      "Companies worked at = Microsoft\n",
      "Companies worked at = Microsoft\n",
      "Name = Puneet Bhandari\n"
     ]
    }
   ],
   "source": [
    "## output label and text of predicted entities (in \"ents\" variable of the spacy doc created above)\n",
    "print(\"PREDICTED:\")\n",
    "for ent in doc.ents:\n",
    "    print('{} = {}'.format(ent.label_, ent.text))\n",
    "\n",
    "## output labeled entities (in \"entities\" dictionary of resume)\n",
    "print(\"\\nLABELED:\")\n",
    "for ent in resume[1]['entities']:\n",
    "    print('{} = {}'.format(ent[2], resume[0][ent[0]:ent[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics for NER\n",
    "Now that we can predict entities using our trained model, we can compare our predictions with the original annotations in our training data to evaluate how well our model performs for our task. The original annotations have been annotated manually by human annotators, and represent a \"Gold Standard\" against which we can compare our predictions. \n",
    "\n",
    "For most classification tasks, the most common evaluation metrics are:\n",
    "* accuracy\n",
    "* precision\n",
    "* recall\n",
    "* f1 score\n",
    "\n",
    "In order to understand these metrics, we need to understand the following concepts:\n",
    "* True positives - How many of the predicted entities are \"true\" according to the Gold Standard? (training annotation) \n",
    "* True negatives - How many entities did the model not predict which are actually not entities according to the Gold Standard?\n",
    "* False positives - How many entities did the model predict which are NOT entities according to the Gold Standard?  \n",
    "* False negatives - How many entities did the model \"miss\" - e.g. did not recognize as entities which are entities according to the Gold Standard? \n",
    "\n",
    "Before we go on, it is important to understand true/false positives/negatives as well as the evaluation metrics above.\n",
    "\n",
    "##### How are the evaluation metrics above defined in the context of evaluating Machine Learning models? How do they relate to True/False Positives/Negatives above?\n",
    "* accuracy: fraction of correctly classificatied samples\n",
    "  * accuracy = (tp + tn) / \\#samples\n",
    "* precision: measure of exactness (how successful in distinguishing between positives and negatives)\n",
    "  * precision = tp / (tp + fp)\n",
    "* recall: measure of completeness (how successful in recognizing all positives)\n",
    "  * recall = tp / (tp + fn)\n",
    "* f1 score: harmonic mean of precision and recall, optimizing for both precision and recall\n",
    "  * f1 score = 2 \\* precision \\* recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Metrics based on token-level annotations or full entity-level. \n",
    "The concepts above are our first step toward understanding how to evaluate our model effectively. However, in NER, we need to take into account that we can calculate our metrics either based on all tokens (words) found in the document, or only on the entities found in the document.  \n",
    "\n",
    "##### Token-Level evaluation. \n",
    "Token level evaluation evaluates how accurately did the model tag *each individual word/token* in the input. In order to understand this, we need to understand something called the \"BILUO\" Scheme (or BILOU or BIO).\n",
    "\n",
    "https://spacy.io/api/annotation#biluo\n",
    "\n",
    "##### Conversion from offsets to BILOU format\n",
    "Up to now, we have not been working with the BILUO scheme, but with \"offsets\" (for example: (112,150,\"Email\") - which says there is an \"Email\" entity between positions 112 and 150 in the text). We would like to be able to evaluate our models on a token-level using BILUO - so we need to convert our data to BILUO. Fortunately, Spacy provides a helper method to do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Puneet</td>\n",
       "      <td>B-Name</td>\n",
       "      <td>B-Name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bhandari</td>\n",
       "      <td>L-Name</td>\n",
       "      <td>L-Name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SAP</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SD</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lead</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>B-Companies worked at</td>\n",
       "      <td>U-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IT</td>\n",
       "      <td>L-Companies worked at</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Pune</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Email</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>me</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Indeed</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>indeed.com/r/Puneet-Bhandari/c9002fa44d6760bd</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Willing</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>relocate</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Anywhere</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>WORK</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>EXPERIENCE</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>Higher</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>Secondary</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>Certificate</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>Shanti</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>Nagar</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>High</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>School</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>board</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>Mumbai</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>1999</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>2000</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>SKILLS</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>Sap</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>Sd</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>(</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>7</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>years</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>)</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1623 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Tokens              Predicted  \\\n",
       "0                                            Puneet                 B-Name   \n",
       "1                                          Bhandari                 L-Name   \n",
       "2                                                                        O   \n",
       "3                                               SAP                      O   \n",
       "4                                                SD                      O   \n",
       "5                                              lead                      O   \n",
       "6                                                 -                      O   \n",
       "7                                         Microsoft  B-Companies worked at   \n",
       "8                                                IT  L-Companies worked at   \n",
       "9                                                                        O   \n",
       "10                                             Pune                      O   \n",
       "11                                                ,                      O   \n",
       "12                                      Maharashtra                      O   \n",
       "13                                                -                      O   \n",
       "14                                            Email                      O   \n",
       "15                                               me                      O   \n",
       "16                                               on                      O   \n",
       "17                                           Indeed                      O   \n",
       "18                                                :                      O   \n",
       "19    indeed.com/r/Puneet-Bhandari/c9002fa44d6760bd                      O   \n",
       "20                                                                       O   \n",
       "21                                          Willing                      O   \n",
       "22                                               to                      O   \n",
       "23                                         relocate                      O   \n",
       "24                                                :                      O   \n",
       "25                                         Anywhere                      O   \n",
       "26                                                                       O   \n",
       "27                                             WORK                      O   \n",
       "28                                       EXPERIENCE                      O   \n",
       "29                                                                       O   \n",
       "...                                             ...                    ...   \n",
       "1593                                                                     O   \n",
       "1594                                         Higher                      O   \n",
       "1595                                      Secondary                      O   \n",
       "1596                                    Certificate                      O   \n",
       "1597                                                                     O   \n",
       "1598                                         Shanti                      O   \n",
       "1599                                          Nagar                      O   \n",
       "1600                                           High                      O   \n",
       "1601                                         School                      O   \n",
       "1602                                              ,                      O   \n",
       "1603                                    Maharashtra                      O   \n",
       "1604                                          board                      O   \n",
       "1605                                              -                      O   \n",
       "1606                                                                     O   \n",
       "1607                                         Mumbai                      O   \n",
       "1608                                              ,                      O   \n",
       "1609                                    Maharashtra                      O   \n",
       "1610                                                                     O   \n",
       "1611                                           1999                      O   \n",
       "1612                                             to                      O   \n",
       "1613                                           2000                      O   \n",
       "1614                                                                     O   \n",
       "1615                                         SKILLS                      O   \n",
       "1616                                                                     O   \n",
       "1617                                            Sap                      O   \n",
       "1618                                             Sd                      O   \n",
       "1619                                              (                      O   \n",
       "1620                                              7                      O   \n",
       "1621                                          years                      O   \n",
       "1622                                              )                      O   \n",
       "\n",
       "                       True  \n",
       "0                    B-Name  \n",
       "1                    L-Name  \n",
       "2                         O  \n",
       "3                         O  \n",
       "4                         O  \n",
       "5                         O  \n",
       "6                         O  \n",
       "7     U-Companies worked at  \n",
       "8                         O  \n",
       "9                         O  \n",
       "10                        O  \n",
       "11                        O  \n",
       "12                        O  \n",
       "13                        O  \n",
       "14                        O  \n",
       "15                        O  \n",
       "16                        O  \n",
       "17                        O  \n",
       "18                        O  \n",
       "19                        O  \n",
       "20                        O  \n",
       "21                        O  \n",
       "22                        O  \n",
       "23                        O  \n",
       "24                        O  \n",
       "25                        O  \n",
       "26                        O  \n",
       "27                        O  \n",
       "28                        O  \n",
       "29                        O  \n",
       "...                     ...  \n",
       "1593                      O  \n",
       "1594                      O  \n",
       "1595                      O  \n",
       "1596                      O  \n",
       "1597                      O  \n",
       "1598                      O  \n",
       "1599                      O  \n",
       "1600                      O  \n",
       "1601                      O  \n",
       "1602                      O  \n",
       "1603                      O  \n",
       "1604                      O  \n",
       "1605                      O  \n",
       "1606                      O  \n",
       "1607                      O  \n",
       "1608                      O  \n",
       "1609                      O  \n",
       "1610                      O  \n",
       "1611                      O  \n",
       "1612                      O  \n",
       "1613                      O  \n",
       "1614                      O  \n",
       "1615                      O  \n",
       "1616                      O  \n",
       "1617                      O  \n",
       "1618                      O  \n",
       "1619                      O  \n",
       "1620                      O  \n",
       "1621                      O  \n",
       "1622                      O  \n",
       "\n",
       "[1623 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy.gold import biluo_tags_from_offsets\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "## returns a pandas dataframe with tokens, prediction, and true (Gold Standard) annotations of tokens\n",
    "def make_bilou_df(nlp,resume):\n",
    "    \"\"\"\n",
    "    param nlp - a trained spacy model\n",
    "    param resume - a resume from our train or test set\n",
    "    \"\"\"\n",
    "    doc = nlp(resume[0])\n",
    "    bilou_ents_predicted = biluo_tags_from_offsets(doc, [(ent.start_char,ent.end_char,ent.label_) for ent in doc.ents])\n",
    "    bilou_ents_true = biluo_tags_from_offsets(doc, [(ent[0], ent[1], ent[2]) for ent in resume[1][\"entities\"]])\n",
    "\n",
    "    doc_tokens = [tok.text for tok in doc]\n",
    "    bilou_df = pd.DataFrame()\n",
    "    bilou_df[\"Tokens\"] = doc_tokens\n",
    "    bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(r\"\\s+\",\"\") \n",
    "    bilou_df[\"Predicted\"] = bilou_ents_predicted\n",
    "    bilou_df[\"True\"] = bilou_ents_true\n",
    "    return bilou_df\n",
    "\n",
    "bilou_df = make_bilou_df(custom_nlp, resume)\n",
    "display(bilou_df)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this output, it should be very easy to calculate a token-level accuracy. We simply compare the \"Predicted\" to \"True\" columns and calculate what percentage are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1616/1623 tokens have been predicted correctly\n",
      "Accuracy on one resume:  0.9956869993838571\n"
     ]
    }
   ],
   "source": [
    "same_df = bilou_df.loc[lambda df: df[\"Predicted\"] == df[\"True\"], :]\n",
    "print('{}/{} tokens have been predicted correctly'.format(len(same_df), len(bilou_df)))\n",
    "accuracy = len(same_df) / len(bilou_df)\n",
    "print(\"Accuracy on one resume: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>B-Companies worked at</td>\n",
       "      <td>U-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IT</td>\n",
       "      <td>L-Companies worked at</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>B-Companies worked at</td>\n",
       "      <td>U-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>IT</td>\n",
       "      <td>L-Companies worked at</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>O</td>\n",
       "      <td>U-Companies worked at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>IIT</td>\n",
       "      <td>B-Companies worked at</td>\n",
       "      <td>B-College Name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>Roorkee</td>\n",
       "      <td>L-Companies worked at</td>\n",
       "      <td>L-College Name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Tokens              Predicted                   True\n",
       "7     Microsoft  B-Companies worked at  U-Companies worked at\n",
       "8            IT  L-Companies worked at                      O\n",
       "34    Microsoft  B-Companies worked at  U-Companies worked at\n",
       "35           IT  L-Companies worked at                      O\n",
       "1287  Microsoft                      O  U-Companies worked at\n",
       "1536        IIT  B-Companies worked at         B-College Name\n",
       "1537    Roorkee  L-Companies worked at         L-College Name"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find all rows in bilou_df where \"Predicted\" not equal to \"True\" column. \n",
    "diff_df = bilou_df.loc[bilou_df[\"Predicted\"] != bilou_df[\"True\"]]\n",
    "display(diff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the accuracy on all our test resumes and average them for an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9843059593260234\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "doc_accuracy = []\n",
    "for res in test:\n",
    "    # calculate accuracy for each test resume and append to doc_accuracy list \n",
    "    bilou_df = make_bilou_df(custom_nlp, res)\n",
    "    same_df = same_df = bilou_df.loc[bilou_df[\"Predicted\"] == bilou_df[\"True\"]]\n",
    "    accuracy = accuracy = len(same_df) / len(bilou_df)\n",
    "    doc_accuracy.append(accuracy)\n",
    "\n",
    "total_acc = np.mean(doc_accuracy)\n",
    "print(\"Accuracy: \",total_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does the model perform on token-level accuracy? What did it miss? In those cases where the predictions didn't match the gold standard, were the predictions plausible or just \"spurious\" (wrong)? \n",
    "Performs generally quite well. Some false positives. Some false negatives. Some cases of token boundaries not matching offsets of labeled entities (problem with input data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What might the advantages and disadvantages be of calculating accuracy on token-level?\n",
    "Token-level: majority of tokens are non-entities -> trivial to reach high accuracy. But proper entity recognition is \n",
    "(almost) not reflected in the high accuracy score. Pro: easy to calculate? Fits into the standard tp/tn/fp/fn schema of classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entity-Level evaluation #####\n",
    "Another method of evaluating the performance of our NER model is to calculate metrics not on token-level, but on entity level. There is a good blog article that describes this method. \n",
    "\n",
    "http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n",
    "\n",
    "The article goes into some detail, the most important part is the scenarios described in the section \"Comparing NER system output and golden standard\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do the first 3 scenarios described in the section \"Comparing NER system output and golden standard\" correlate to  true/false positives/negatives? \n",
    "1. string and entity type match: true positive\n",
    "2. entity imagined: false positive\n",
    "3. entity missed: false negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Precision, Recall, F1 #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to calculate precision, recall, and f1 for each entity type we are interested in (our chosen entities). To do this, we need to understand the formulas for each. A good article for this is https://skymind.ai/wiki/accuracy-precision-recall-f1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How can we calculate precision, recall and f1 score based on the information above?\n",
    "* precision = tp / (tp + fp)\n",
    "* recall = tp / (tp + fn)\n",
    "* f1 = 2 * prec * rec / (prec + rec)\n",
    "* tp = entities recognized as entities\n",
    "* tn = non-entities recognized as non-entities\n",
    "* fp = non-entities recognized as entities\n",
    "* fn = entities recognized as non-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For label 'Name' tp: 526 fp: 446 fn: 86\n",
      "Precision:  0.5411522633744856\n",
      "Recall:  0.8594771241830066\n",
      "F1:  0.6641414141414141\n",
      "For label 'College Name' tp: 354 fp: 446 fn: 84\n",
      "Precision:  0.4425\n",
      "Recall:  0.8082191780821918\n",
      "F1:  0.5718901453957996\n",
      "For label 'Companies worked at' tp: 834 fp: 191 fn: 116\n",
      "Precision:  0.8136585365853658\n",
      "Recall:  0.8778947368421053\n",
      "F1:  0.8445569620253164\n"
     ]
    }
   ],
   "source": [
    "# cycle through chosen_entity_labels and calculate metrics for each entity using test data\n",
    "metrics_data = []\n",
    "for label in chosen_entity_labels:\n",
    "    ## sums for all resumes for one entity type\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for tres in test:\n",
    "        tres_df = make_bilou_df(custom_nlp, tres)\n",
    "        ## calculate true / false positives / negatives for each resume\n",
    "        tp = ((tres_df['True'].str.endswith(label)) & (tres_df['Predicted'] == tres_df['True'])).sum()\n",
    "        fp = ((tres_df['Predicted'].str.endswith(label)) & (tres_df['True'] == 'O')).sum()\n",
    "        fn = ((tres_df['True'].str.endswith(label)) & (tres_df['Predicted'] == 'O')).sum()\n",
    "        ## aggregate results\n",
    "        true_positives += tp\n",
    "        false_positives += fp\n",
    "        false_negatives += fn\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(\"For label '{}' tp: {} fp: {} fn: {}\".format(label,true_positives,false_positives,false_negatives))\n",
    "    print(\"Precision: \",precision)\n",
    "    print(\"Recall: \",recall)\n",
    "    print(\"F1: \",f1)\n",
    "    row = [precision,recall,f1]\n",
    "    metrics_data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute an average score for each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <td>0.541152</td>\n",
       "      <td>0.859477</td>\n",
       "      <td>0.664141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>College Name</th>\n",
       "      <td>0.442500</td>\n",
       "      <td>0.808219</td>\n",
       "      <td>0.571890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Companies worked at</th>\n",
       "      <td>0.813659</td>\n",
       "      <td>0.877895</td>\n",
       "      <td>0.844557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entities averaged</th>\n",
       "      <td>0.599104</td>\n",
       "      <td>0.848530</td>\n",
       "      <td>0.693530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Precision    Recall        F1\n",
       "Name                  0.541152  0.859477  0.664141\n",
       "College Name          0.442500  0.808219  0.571890\n",
       "Companies worked at   0.813659  0.877895  0.844557\n",
       "entities averaged     0.599104  0.848530  0.693530"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute average metrics\n",
    "columns = ['Precision', 'Recall', 'F1']\n",
    "metrics_df = pd.DataFrame(data=metrics_data, index=chosen_entity_labels, columns=columns)\n",
    "metrics_df.loc['entities averaged'] = [metrics_df[m].mean() for m in columns]\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do the average metrics here (computed on entity-level) compare to the token-level accuracy score above?\n",
    "The average metrics here are macro-averaged, i.e. averaged at entity-level. Rarely occuring entities have a much higher influence than with micro-averaged metrics, i.e. metrics at token-level.\n",
    "\n",
    "At the end I would prefer entity-level metrics to make it obvious if some entities (even if they are rare) perform badly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save BILUO training data for reuse in part 3\n",
    "For part 3 we are using flair and loading our data from a .csv file into a flair \"Corpus\". This is described here:\n",
    "\n",
    "https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md#reading-your-own-sequence-labeling-dataset\n",
    "\n",
    "We need to create \"train\" and \"test\" .csv files using our train and test dataset which corresponds to the format described above. This format is one line containing (minimally) a text token and a NER Tag. These should be separated by whitespace. (Which means entity names must not contain whitespace!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentences\n",
    "Flair works with \"Sentences\" which is a list of tokens. If we simply write out our csv with one line for every token in our dataset, we will have 1 giant sentence with many thousands of words.. This is not what we want. \n",
    "We would like to partition our data so that we have a list of \"Sentences\" - corresponding to our intuition for a sentence - a sequence of words that belong together and is not all to long, usually separated by some punctuation. \n",
    "When we create our .csv strings/files, we need to do so so that they represent a list of sentences, each sentence consisting of a list of tokens/tags (each token/tag being one line in our csv). \n",
    "\n",
    "***To do this, we create a blank newline after each sentence.*** (1 line per token, empty line between sentences)\n",
    "\n",
    "Each csv file will be a (long) list of token/tag lines, with sentences separated by newlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make bilou dfs\n",
      "Done!\n",
      "16521 sentences\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "def export_bilou_df(df_idx, df, filehandle, sentence_chars='.*;', column_delimiter='\\t', row_delimiter='\\n'):\n",
    "    sentence_counter = 1\n",
    "    # rename entities (must not contain whitespace)\n",
    "    for col in ['Predicted', 'True']:\n",
    "        df[col] = df[col].str.replace(' ', '_', regex=False)\n",
    "    for i, row in enumerate(df.itertuples(index=False, name=None)):\n",
    "        token = row[0]\n",
    "        true_bilou_tag = row[2]\n",
    "        if len(token) == 0 or token.isspace():\n",
    "            continue  # ignore newlines or whitespace\n",
    "        filehandle.write(str(df_idx))\n",
    "        filehandle.write(column_delimiter)\n",
    "        filehandle.write(column_delimiter.join(row))\n",
    "        filehandle.write(row_delimiter)\n",
    "        # was the token an end-of-sentence marker?\n",
    "        # and not the last token of the df\n",
    "        # and not with BILOU tag B-... or I-... (happens in abbreviations inside names)\n",
    "        # ^- commented out; reason: would leak information about gold standard to model\n",
    "        if token in sentence_chars and i < len(df) - 1:  # and not true_bilou_tag.startswith(('B-', 'I-')):\n",
    "            sentence_counter += 1\n",
    "            # insert a blank line\n",
    "            filehandle.write(row_delimiter)\n",
    "    # write a final blank line (to enforce sentence boundary not crossing df boundary)\n",
    "    filehandle.write(row_delimiter)\n",
    "    # print('df {}: {} sentences'.format('?', sentence_counter))\n",
    "    return sentence_counter\n",
    "\n",
    "def bilou_for_flair(nlp, train, test):\n",
    "    \"\"\"\n",
    "    make .csv strings from train and test for use in flair\n",
    "    \"\"\"    \n",
    "    print(\"Make bilou dfs\")\n",
    "    # makes a list of pandas dataframes, one for each resume. \n",
    "    training_data_as_bilou = [make_bilou_df(nlp, res) for res in train]\n",
    "    test_data_as_bilou = [make_bilou_df(nlp, res) for res in test]\n",
    "    print(\"Done!\")\n",
    "    # strings to return\n",
    "    training_file = io.StringIO()\n",
    "    test_file = io.StringIO()\n",
    "    sentence_counter = 0\n",
    "    for df_idx, df in enumerate(training_data_as_bilou):\n",
    "        sentence_counter += export_bilou_df(df_idx, df, training_file)\n",
    "    for df_idx, df in enumerate(test_data_as_bilou):\n",
    "        sentence_counter += export_bilou_df(df_idx, df, test_file)\n",
    "    print('{} sentences'.format(sentence_counter))\n",
    "    return training_file.getvalue(), test_file.getvalue()\n",
    "\n",
    "training_str, test_str = bilou_for_flair(custom_nlp, train, test)\n",
    "with open(\"flair/train_res_bilou.txt\", 'w+', encoding=\"utf-8\") as f:\n",
    "    f.write(training_str)\n",
    "with open(\"flair/test_res_bilou.txt\",'w+',encoding=\"utf-8\") as f:\n",
    "    f.write(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data we persisted with flair before we go on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-19 02:43:29,248 Reading data from flair\n",
      "2019-06-19 02:43:29,250 Train: flair/train_res_bilou.txt\n",
      "2019-06-19 02:43:29,251 Dev: None\n",
      "2019-06-19 02:43:29,252 Test: flair/test_res_bilou.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/Documents/SAKI/venv/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  app.launch_new_instance()\n",
      "/home/daniel/Documents/SAKI/venv/lib/python3.6/site-packages/flair/data_fetcher.py:312: DeprecationWarning: Call to deprecated function (or staticmethod) read_column_data. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  train_file, column_format\n",
      "/home/daniel/Documents/SAKI/venv/lib/python3.6/site-packages/flair/data_fetcher.py:318: DeprecationWarning: Call to deprecated function (or staticmethod) read_column_data. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
      "  test_file, column_format\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 11116 train + 1235 dev + 4170 test sentences\n"
     ]
    }
   ],
   "source": [
    "# requires flair 0.4.2\n",
    "from flair.datasets import Corpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "\n",
    "# folder where training and test data are\n",
    "data_folder = 'flair'\n",
    "# your training file name\n",
    "train_file = 'train_res_bilou.txt'\n",
    "# your training file name\n",
    "test_file = 'test_res_bilou.txt'\n",
    "\n",
    "columns = {1:'text', 3:'ner'}\n",
    "\n",
    "## Now load our csv into flair corpus\n",
    "corpus: Corpus = NLPTaskDataFetcher.load_column_corpus(\n",
    "    data_folder, columns, train_file=train_file, test_file=test_file, dev_file=None\n",
    ")\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you could load the corpus without error, you are ready to go on to part 3, where we will work with flair nlp! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
