{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem_description_and_state import ProblemDescription, State\n",
    "from data_exploration_classes import Data\n",
    "from training_classes import TransitionProbabilityLearner, TransitionProbabilityMatrix, RewardMatrixS1, RewardMatrixSA\n",
    "from evaluation_classes import GreedyPolicy, PolicyFromMDPSolver, Evaluation\n",
    "import collections\n",
    "import datetime\n",
    "import mdptoolbox\n",
    "import numpy\n",
    "\n",
    "Problem = collections.namedtuple('Problem', ['pd', 'data', 'learner', 'tpms', 'rm', 'greedy_scores', 'results'])\n",
    "Result = collections.namedtuple('Result', [\n",
    "    'solver', 'discount_factor', 'max_iter', 'time_real', 'time_solver', 'policy', 'scores'])\n",
    "\n",
    "def create_problem(cols, rows, datafiles):\n",
    "    pd = ProblemDescription(cols, rows)\n",
    "    data = [Data(pd, datafile) for datafile in datafiles]\n",
    "    # learn transition probabilities from the training data\n",
    "    learner = TransitionProbabilityLearner(pd, data[0], min_support=5)\n",
    "    # create the transition probability matrices\n",
    "    tpms = list(TransitionProbabilityMatrix(pd, learner, dtype=numpy.float64))\n",
    "    # tried other dtypes as well, but float32 and float16 produced errors in mdptoolbox\n",
    "    # (incompatible datatypes for float16, t.p.m. not stochastic for float32)\n",
    "    rm = RewardMatrixSA(pd, dtype=numpy.int16).get()\n",
    "    # SA stands for the shape of the reward matrix, i.e. one matrix with shape SxA\n",
    "    # instead of a list of multiple matrices of shape Sx1 for each action (which didn't work with mdptoolbox)\n",
    "    return Problem(pd, data, learner, tpms, rm, [], [])\n",
    "\n",
    "p4 = create_problem(2, 2, ['data/2x2/warehouse{}.txt'.format(s) for s in ['training', 'order', 'ordernew']])\n",
    "p6 = create_problem(3, 2, ['data/3x2/warehouse{}.txt'.format(s) for s in ['training', 'order']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing a Performance Reference through a Greedy Algorithm\n",
    "\n",
    "A greedy algorithm always uses the closest applicable slot to satisfy a request. It has no notion of planning and does not care for the distribution of item colors.\n",
    "\n",
    "Comparing its sum of rewards over a series of requests given as training data or test data can serve to establish a reference value to compare optimal policies as solutions from a MDP to. Instead of the sum of rewards derived values like average reward per request or normalized score values can be used as well.\n",
    "\n",
    "One such score is defined as follows:\n",
    "* compute the average reward per request (= sum of rewards / #requests)\n",
    "  * note: rewards are integers in the range of (-infty, 0]\n",
    "* divide that by the minimum (i.e. worst) reward under normal circumstances (i.e. without invalid states, invalid requests or invalid actions)\n",
    "* multiply by -1 to get a negative number again\n",
    "* add 1\n",
    "* replace any remaining negative values (caused by invalid states etc.) by 0.0\n",
    "* the resulting score is now in the range \\[0.0, 1.0\\]\n",
    "* 0.0 is the worst achievable performance under normal circumstances, 1.0 is perfect performance (only achievable given optimal requests, i.e. only need to use the last inventory slot for all requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = p6.pd  # use the 3x2 inventory\n",
    "gpol = GreedyPolicy(pd)  \n",
    "verb_store = 0\n",
    "verb_restore = 1\n",
    "color1 = 0\n",
    "last_slot = pd.number_of_inventory_slots - 1\n",
    "# inventory slot numbers:\n",
    "# 0  1  2\n",
    "# 3  4  5 <-+\n",
    "#       ^---+-- last slot, highest reward (=0)\n",
    "assert gpol.get_action(State([0,0,0,0,0,0], verb_store, color1).get_index(pd)) == last_slot\n",
    "assert gpol.get_action(State([0,0,0,0,1,1], verb_store, color1).get_index(pd)) == 2\n",
    "assert gpol.get_action(State([0,1,1,0,1,1], verb_store, color1).get_index(pd)) == 3\n",
    "assert gpol.get_action(State([0,1,1,1,1,1], verb_store, color1).get_index(pd)) == 0\n",
    "\n",
    "assert gpol.get_action(State([1,1,1,1,1,1], verb_restore, color1).get_index(pd)) == last_slot\n",
    "assert gpol.get_action(State([1,1,1,1,0,0], verb_restore, color1).get_index(pd)) == 2\n",
    "assert gpol.get_action(State([1,0,0,1,0,0], verb_restore, color1).get_index(pd)) == 3\n",
    "assert gpol.get_action(State([1,0,0,0,0,0], verb_restore, color1).get_index(pd)) == 0\n",
    "\n",
    "assert gpol.get_action(State([2,2,2,1,1,1], verb_restore, color1 + 1).get_index(pd)) == 2\n",
    "assert gpol.get_action(State([1,1,1,2,0,0], verb_restore, color1 + 1).get_index(pd)) == 3\n",
    "\n",
    "ignore_request_action = pd.number_of_actions - 1\n",
    "assert gpol.get_action(State([1,1,1,1,1,1], verb_store, color1).get_index(pd)) == ignore_request_action\n",
    "assert gpol.get_action(State([2,2,2,2,2,2], verb_store, color1).get_index(pd)) == ignore_request_action\n",
    "assert gpol.get_action(State([2,2,2,2,2,2], verb_restore, color1).get_index(pd)) == ignore_request_action\n",
    "assert gpol.get_action(State([0,0,0,0,0,0], verb_restore, color1).get_index(pd)) == ignore_request_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Greedy Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using data from data/3x2/warehousetraining.txt with 12108 requests\n",
      "- total reward -13578\n",
      "- average reward per request -1.121\n",
      "- minimum achievable reward per request -3.000\n",
      "- score 0.626\n",
      "\n",
      "using data from data/3x2/warehouseorder.txt with 60 requests\n",
      "- total reward -64\n",
      "- average reward per request -1.067\n",
      "- minimum achievable reward per request -3.000\n",
      "- score 0.644\n",
      "\n",
      "using data from data/2x2/warehousetraining.txt with 8177 requests\n",
      "- total reward -6224\n",
      "- average reward per request -0.761\n",
      "- minimum achievable reward per request -2.000\n",
      "- score 0.619\n",
      "\n",
      "using data from data/2x2/warehouseorder.txt with 65 requests\n",
      "- total reward -49\n",
      "- average reward per request -0.754\n",
      "- minimum achievable reward per request -2.000\n",
      "- score 0.623\n",
      "\n",
      "using data from data/2x2/warehouseordernew.txt with 20 requests\n",
      "- total reward -19\n",
      "- average reward per request -0.950\n",
      "- minimum achievable reward per request -2.000\n",
      "- score 0.525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for problem in [p6, p4]:\n",
    "    gpol = GreedyPolicy(problem.pd)\n",
    "    problem.greedy_scores.clear()\n",
    "    for data in problem.data:\n",
    "        eva = Evaluation(problem.pd, data, gpol)\n",
    "        problem.greedy_scores.append(eva.get_score())\n",
    "        print('using data from {} with {} requests'.format(data.filepath, len(data.requests)))\n",
    "        print('- total reward {}'.format(eva.get_total_reward()))\n",
    "        print('- average reward per request {:.3f}'.format(eva.get_total_reward() / len(data.requests)))\n",
    "        print('- minimum achievable reward per request {:.3f}'.format(min((\n",
    "                -problem.pd.get_manhattan_distance_to_last_inventory_slot(slot)\n",
    "                for slot in range(problem.pd.number_of_inventory_slots)\n",
    "        ))))\n",
    "        print('- score {:.3f}'.format(eva.get_score()))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Greedy Algorithm with an Optimal Policy as Computed by a MDP Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset solver results from previous runs\n",
    "p4.results.clear()\n",
    "p6.results.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_solver(problem, solver_class, discount_factor, iterations, show_training_score_only=True):\n",
    "    start = datetime.datetime.now()\n",
    "    solver = solver_class(\n",
    "        problem.tpms,\n",
    "        problem.rm,\n",
    "        discount_factor,\n",
    "        max_iter=iterations\n",
    "    )\n",
    "    try:\n",
    "        solver.run()\n",
    "    except MemoryError:\n",
    "        print('{}x{}: {}(discount_factor={}, max_iter={}) could not run due to memory issues'.format(\n",
    "            problem.pd.number_of_inventory_cols, problem.pd.number_of_inventory_rows,\n",
    "            type(solver).__name__, discount_factor, iterations\n",
    "        ))\n",
    "        return\n",
    "    stop = datetime.datetime.now()\n",
    "    time_real = (stop - start).total_seconds()\n",
    "    time_solver = solver.time\n",
    "    policy = PolicyFromMDPSolver(problem.pd, solver)\n",
    "    result = Result(type(solver).__name__, discount_factor, iterations, time_real, time_solver, policy, [])\n",
    "    print('{}x{}: {}(discount_factor={}, max_iter={}) ran for {:.1f}s ({:.1f}s including initialization)'.format(\n",
    "        problem.pd.number_of_inventory_cols, problem.pd.number_of_inventory_rows,\n",
    "        result.solver, result.discount_factor, result.max_iter, result.time_solver, result.time_real\n",
    "    ))\n",
    "    max_filenamelength = max((len(data.filepath) for data in problem.data))\n",
    "    for data_index, data in enumerate(problem.data):\n",
    "        score = Evaluation(problem.pd, data, policy).get_score()\n",
    "        result.scores.append(score)\n",
    "        if show_training_score_only and data_index > 0:\n",
    "            continue\n",
    "        print(('\\t{: <' + str(max_filenamelength) + '}: solver score {:.3f} vs greedy score {:.3f}').format(\n",
    "            data.filepath, score, problem.greedy_scores[data_index]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x2: PolicyIteration(discount_factor=0.95, max_iter=10) ran for 2.3s (2.5s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.619 vs greedy score 0.619\n",
      "\tdata/2x2/warehouseorder.txt   : solver score 0.623 vs greedy score 0.623\n",
      "\tdata/2x2/warehouseordernew.txt: solver score 0.525 vs greedy score 0.525\n",
      "3x2: PolicyIteration(discount_factor=0.95, max_iter=10) could not run due to memory issues\n",
      "--------------------------------------------------------------------------------\n",
      "2x2: PolicyIterationModified(discount_factor=0.95, max_iter=10) ran for 0.8s (1.0s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.619 vs greedy score 0.619\n",
      "\tdata/2x2/warehouseorder.txt   : solver score 0.623 vs greedy score 0.623\n",
      "\tdata/2x2/warehouseordernew.txt: solver score 0.525 vs greedy score 0.525\n",
      "3x2: PolicyIterationModified(discount_factor=0.95, max_iter=10) ran for 184.6s (243.2s including initialization)\n",
      "\tdata/3x2/warehousetraining.txt: solver score 0.626 vs greedy score 0.626\n",
      "\tdata/3x2/warehouseorder.txt   : solver score 0.644 vs greedy score 0.644\n",
      "--------------------------------------------------------------------------------\n",
      "2x2: ValueIteration(discount_factor=0.95, max_iter=5000) ran for 0.0s (2.4s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.619 vs greedy score 0.619\n",
      "\tdata/2x2/warehouseorder.txt   : solver score 0.623 vs greedy score 0.623\n",
      "\tdata/2x2/warehouseordernew.txt: solver score 0.525 vs greedy score 0.525\n",
      "3x2: ValueIteration(discount_factor=0.95, max_iter=5000) ran for 0.9s (221.1s including initialization)\n",
      "\tdata/3x2/warehousetraining.txt: solver score 0.626 vs greedy score 0.626\n",
      "\tdata/3x2/warehouseorder.txt   : solver score 0.644 vs greedy score 0.644\n",
      "--------------------------------------------------------------------------------\n",
      "2x2: ValueIterationGS(discount_factor=0.95, max_iter=5000) ran for 142.4s (144.8s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.619 vs greedy score 0.619\n",
      "\tdata/2x2/warehouseorder.txt   : solver score 0.623 vs greedy score 0.623\n",
      "\tdata/2x2/warehouseordernew.txt: solver score 0.525 vs greedy score 0.525\n",
      "3x2: ValueIterationGS skipped due to excessive runtime\n",
      "--------------------------------------------------------------------------------\n",
      "2x2: RelativeValueIteration(discount_factor=0.95, max_iter=5000) ran for 1.1s (1.3s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.619 vs greedy score 0.619\n",
      "\tdata/2x2/warehouseorder.txt   : solver score 0.623 vs greedy score 0.623\n",
      "\tdata/2x2/warehouseordernew.txt: solver score 0.525 vs greedy score 0.525\n",
      "3x2: RelativeValueIteration(discount_factor=0.95, max_iter=5000) ran for 24.8s (84.9s including initialization)\n",
      "\tdata/3x2/warehousetraining.txt: solver score 0.626 vs greedy score 0.626\n",
      "\tdata/3x2/warehouseorder.txt   : solver score 0.644 vs greedy score 0.644\n"
     ]
    }
   ],
   "source": [
    "evaluate_solver(p4, mdptoolbox.mdp.PolicyIteration, 0.95, 10, False)\n",
    "evaluate_solver(p6, mdptoolbox.mdp.PolicyIteration, 0.95, 10, False)\n",
    "print('-' * 80)\n",
    "evaluate_solver(p4, mdptoolbox.mdp.PolicyIterationModified, 0.95, 10, False)\n",
    "evaluate_solver(p6, mdptoolbox.mdp.PolicyIterationModified, 0.95, 10, False)\n",
    "print('-' * 80)\n",
    "evaluate_solver(p4, mdptoolbox.mdp.ValueIteration, 0.95, 5000, False)\n",
    "evaluate_solver(p6, mdptoolbox.mdp.ValueIteration, 0.95, 5000, False)\n",
    "print('-' * 80)\n",
    "evaluate_solver(p4, mdptoolbox.mdp.ValueIterationGS, 0.95, 5000, False)\n",
    "# evaluate_solver(p6, mdptoolbox.mdp.ValueIterationGS, 0.95, 5000, False)\n",
    "print('3x2: ValueIterationGS skipped due to excessive runtime')\n",
    "print('-' * 80)\n",
    "evaluate_solver(p4, mdptoolbox.mdp.RelativeValueIteration, 0.95, 5000, False)\n",
    "evaluate_solver(p6, mdptoolbox.mdp.RelativeValueIteration, 0.95, 5000, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "All MDP algorithms produce policies which perform exactly as good as a greedy policy when executed on the request sequences of any of the provided data files. Given that the parameters like discount factor and number of iterations are already quite reasonable default values for these algorithms the implication is that there simply is no better policy for such request sequences.\n",
    "\n",
    "This further implies that it is not beneficial for the agent to plan ahead for the patterns discovered in the data exploration notebook. Reverse engineering the generator function that produced the data files showed (disregarding the sequential request patterns) that the probability of a restore request is roughly (#occupied slots) / (#inventory slots), and that the probability of the color of the item to be restored is roughly (#items of that color in the inventory) / (#occupied slots). This explains why a greedy policy is already optimal. Even though red items are twice as likely to appear as blue or white, their retrieval frequency seems to mostly depend on their color ratio in the inventory. In other words, storing a blue item further away from the starting position than a red item makes no sense, because all items in the inventory are equally likely to be retrieved.\n",
    "\n",
    "There is further evidence to back this up: tweaking the `min_support` parameter of the TransitionProbabilityLearner class by raising it from its default value of 5 to, for example, `1e10` forces the class for practically all state transitions to fall back to the least granular data available (totally disregarding the current inventory content) and to _only use the color distribution_ among all requests to determine the probability of state transitions (verbs are assumed to be equally distributed). Under this condition the agent actually plans ahead _worse_ when the discount factor is _increased_, because its decisions are based on bad data. A quick demonstration with the 2x2 problem and the PolicyIteration solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x2: PolicyIteration(discount_factor=0.2, max_iter=10) ran for 2.2s (2.4s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.619 vs greedy score 0.619\n",
      "2x2: PolicyIteration(discount_factor=0.6, max_iter=10) ran for 2.2s (2.4s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.619 vs greedy score 0.619\n",
      "2x2: PolicyIteration(discount_factor=0.8, max_iter=10) ran for 2.2s (2.4s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.618 vs greedy score 0.619\n",
      "2x2: PolicyIteration(discount_factor=0.9, max_iter=10) ran for 2.2s (2.3s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.616 vs greedy score 0.619\n",
      "2x2: PolicyIteration(discount_factor=0.95, max_iter=10) ran for 2.2s (2.4s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.613 vs greedy score 0.619\n",
      "2x2: PolicyIteration(discount_factor=0.99, max_iter=10) ran for 2.2s (2.4s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.598 vs greedy score 0.619\n",
      "2x2: PolicyIteration(discount_factor=0.999, max_iter=10) ran for 2.2s (2.3s including initialization)\n",
      "\tdata/2x2/warehousetraining.txt: solver score 0.596 vs greedy score 0.619\n"
     ]
    }
   ],
   "source": [
    "learner_mod = TransitionProbabilityLearner(p4.pd, p4.data[0], min_support=1e10)\n",
    "tpms_mod = list(TransitionProbabilityMatrix(p4.pd, learner_mod, dtype=numpy.float64))\n",
    "p4_mod = Problem(p4.pd, p4.data, learner_mod, tpms_mod, p4.rm, p4.greedy_scores, [])\n",
    "for discount_factor in [0.2, 0.6, 0.8, 0.9, 0.95, 0.99, 0.999]:\n",
    "    evaluate_solver(p4_mod, mdptoolbox.mdp.PolicyIteration, discount_factor, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Discussion of the MDP Algorithms Based on Generated Data\n",
    "\n",
    "The existing data is not suitable to demonstrate the influence of parameter choice on convergence of the algorithms. Data generated with different characteristics can change that.\n",
    "\n",
    "The characteristics of the requests generated by the ModifiedRequestGenerator class differ from those of the existing data in that red is 4 times as likely -- even for restore requests -- and the color probability of restore requests does not depend on the number of items of that color in the inventory (except that at least one item of that color needs to exist). It is also missing any sequential patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_exploration_classes import ModifiedRequestGenerator, GeneratedData\n",
    "\n",
    "data_gen = GeneratedData(p4.pd)\n",
    "data_gen.requests = ModifiedRequestGenerator(p4.pd, data_gen.limit)\n",
    "learner_gen = TransitionProbabilityLearner(p4.pd, data_gen, min_support=5)\n",
    "tpms_gen = list(TransitionProbabilityMatrix(p4.pd, learner_gen, dtype=numpy.float64))\n",
    "gpol_gen = GreedyPolicy(p4.pd)\n",
    "gpol_gen_eval = Evaluation(p4.pd, data_gen, gpol_gen)\n",
    "p4_gen = Problem(p4.pd, [data_gen], learner_gen, tpms_gen, p4.rm, [gpol_gen_eval.get_score()], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof and Influence of Parameter Choice\n",
    "\n",
    "#### 1. PolicyIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x2: PolicyIteration(discount_factor=0.95, max_iter=1) ran for 0.2s (0.4s including initialization)\n",
      "\tgenerated data: solver score 0.701 vs greedy score 0.701\n",
      "2x2: PolicyIteration(discount_factor=0.95, max_iter=2) ran for 0.4s (0.6s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: PolicyIteration(discount_factor=0.95, max_iter=4) ran for 0.9s (1.1s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: PolicyIteration(discount_factor=0.95, max_iter=8) ran for 1.8s (1.9s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "--------------------------------------------------------------------------------\n",
      "2x2: PolicyIteration(discount_factor=0.6, max_iter=10) ran for 2.3s (2.5s including initialization)\n",
      "\tgenerated data: solver score 0.701 vs greedy score 0.701\n",
      "2x2: PolicyIteration(discount_factor=0.8, max_iter=10) ran for 2.4s (2.5s including initialization)\n",
      "\tgenerated data: solver score 0.701 vs greedy score 0.701\n",
      "2x2: PolicyIteration(discount_factor=0.9, max_iter=10) ran for 2.3s (2.5s including initialization)\n",
      "\tgenerated data: solver score 0.705 vs greedy score 0.701\n",
      "2x2: PolicyIteration(discount_factor=0.95, max_iter=10) ran for 2.3s (2.4s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: PolicyIteration(discount_factor=0.99, max_iter=10) ran for 2.3s (2.4s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n",
      "2x2: PolicyIteration(discount_factor=0.999, max_iter=10) ran for 2.3s (2.5s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n"
     ]
    }
   ],
   "source": [
    "for max_iter in [1, 2, 4, 8]:\n",
    "    evaluate_solver(p4_gen, mdptoolbox.mdp.PolicyIteration, 0.95, max_iter)\n",
    "print('-' * 80)\n",
    "for discount_factor in [0.6, 0.8, 0.9, 0.95, 0.99, 0.999]:\n",
    "    evaluate_solver(p4_gen, mdptoolbox.mdp.PolicyIteration, discount_factor, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that PolicyIteration does not need more than 10 iterations. It also demonstrates that a discount factor of 0.95 is still a bit low to generate a policy that fully exploits the benefits of long-term planning.\n",
    "\n",
    "#### 2. PolicyIterationModified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x2: PolicyIterationModified(discount_factor=0.95, max_iter=1) ran for 3.1s (3.2s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: PolicyIterationModified(discount_factor=0.95, max_iter=2) ran for 2.1s (2.3s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: PolicyIterationModified(discount_factor=0.95, max_iter=4) ran for 1.3s (1.5s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: PolicyIterationModified(discount_factor=0.95, max_iter=8) ran for 0.9s (1.1s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: PolicyIterationModified(discount_factor=0.95, max_iter=16) ran for 0.6s (0.8s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: PolicyIterationModified(discount_factor=0.95, max_iter=32) ran for 0.4s (0.6s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "--------------------------------------------------------------------------------\n",
      "2x2: PolicyIterationModified(discount_factor=0.6, max_iter=10) ran for 0.1s (0.3s including initialization)\n",
      "\tgenerated data: solver score 0.701 vs greedy score 0.701\n",
      "2x2: PolicyIterationModified(discount_factor=0.8, max_iter=10) ran for 0.1s (0.3s including initialization)\n",
      "\tgenerated data: solver score 0.701 vs greedy score 0.701\n",
      "2x2: PolicyIterationModified(discount_factor=0.9, max_iter=10) ran for 0.3s (0.5s including initialization)\n",
      "\tgenerated data: solver score 0.705 vs greedy score 0.701\n",
      "2x2: PolicyIterationModified(discount_factor=0.95, max_iter=10) ran for 0.8s (1.0s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: PolicyIterationModified(discount_factor=0.99, max_iter=10) ran for 4.5s (4.7s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n",
      "2x2: PolicyIterationModified(discount_factor=0.999, max_iter=10) ran for 55.2s (55.4s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n"
     ]
    }
   ],
   "source": [
    "for max_iter in [1, 2, 4, 8, 16, 32]:\n",
    "    evaluate_solver(p4_gen, mdptoolbox.mdp.PolicyIterationModified, 0.95, max_iter)\n",
    "print('-' * 80)\n",
    "for discount_factor in [0.6, 0.8, 0.9, 0.95, 0.99, 0.999]:\n",
    "    evaluate_solver(p4_gen, mdptoolbox.mdp.PolicyIterationModified, discount_factor, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PolicyIterationModified converges even faster than PolicyIteration. However, it is very sensitive to a high discount factor, i.e. the computation time increases drastically. Curiously, the runtime for very low iterations (only 1 or 2) is higher than for higher iterations (like around 10).\n",
    "\n",
    "#### 3. ValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x2: ValueIteration(discount_factor=0.95, max_iter=1) ran for 0.0s (2.5s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: ValueIteration(discount_factor=0.95, max_iter=100) ran for 0.0s (2.4s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: ValueIteration(discount_factor=0.95, max_iter=10000) ran for 0.0s (2.4s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "--------------------------------------------------------------------------------\n",
      "2x2: ValueIteration(discount_factor=0.6, max_iter=100) ran for 0.0s (2.4s including initialization)\n",
      "\tgenerated data: solver score 0.701 vs greedy score 0.701\n",
      "2x2: ValueIteration(discount_factor=0.8, max_iter=100) ran for 0.0s (2.3s including initialization)\n",
      "\tgenerated data: solver score 0.701 vs greedy score 0.701\n",
      "2x2: ValueIteration(discount_factor=0.9, max_iter=100) ran for 0.0s (2.4s including initialization)\n",
      "\tgenerated data: solver score 0.705 vs greedy score 0.701\n",
      "2x2: ValueIteration(discount_factor=0.95, max_iter=100) ran for 0.0s (2.4s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: ValueIteration(discount_factor=0.99, max_iter=100) ran for 0.2s (2.6s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n",
      "2x2: ValueIteration(discount_factor=0.999, max_iter=100) ran for 2.8s (5.2s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n"
     ]
    }
   ],
   "source": [
    "for max_iter in [1, 100, 10000]:\n",
    "    evaluate_solver(p4_gen, mdptoolbox.mdp.ValueIteration, 0.95, max_iter)\n",
    "print('-' * 80)\n",
    "for discount_factor in [0.6, 0.8, 0.9, 0.95, 0.99, 0.999]:\n",
    "    evaluate_solver(p4_gen, mdptoolbox.mdp.ValueIteration, discount_factor, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that ValueIteration probably ignores the `max_iter` parameter. The result for the discount factor is the same as in PolicyIteration. There is a slight sensitivity to a high discount factor in terms of computation time.\n",
    "\n",
    "#### 4. ValueIterationGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x2: ValueIterationGS(discount_factor=0.95, max_iter=1) ran for 144.5s (146.9s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: ValueIterationGS(discount_factor=0.95, max_iter=10) ran for 144.8s (147.2s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: ValueIterationGS(discount_factor=0.95, max_iter=100) ran for 141.3s (143.6s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: ValueIterationGS(discount_factor=0.95, max_iter=1000) ran for 144.3s (146.7s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "--------------------------------------------------------------------------------\n",
      "2x2: ValueIterationGS(discount_factor=0.6, max_iter=100) ran for 11.5s (13.9s including initialization)\n",
      "\tgenerated data: solver score 0.701 vs greedy score 0.701\n",
      "2x2: ValueIterationGS(discount_factor=0.8, max_iter=100) ran for 28.5s (30.8s including initialization)\n",
      "\tgenerated data: solver score 0.701 vs greedy score 0.701\n",
      "2x2: ValueIterationGS(discount_factor=0.9, max_iter=100) ran for 64.9s (67.3s including initialization)\n",
      "\tgenerated data: solver score 0.705 vs greedy score 0.701\n",
      "2x2: ValueIterationGS(discount_factor=0.95, max_iter=100) ran for 147.2s (149.6s including initialization)\n",
      "\tgenerated data: solver score 0.709 vs greedy score 0.701\n",
      "2x2: ValueIterationGS(discount_factor=0.99, max_iter=100) ran for 875.2s (877.7s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n"
     ]
    }
   ],
   "source": [
    "for max_iter in [1, 10, 100, 1000]:\n",
    "    evaluate_solver(p4_gen, mdptoolbox.mdp.ValueIterationGS, 0.95, max_iter)\n",
    "print('-' * 80)\n",
    "for discount_factor in [0.6, 0.8, 0.9, 0.95, 0.99, 0.999]:\n",
    "    evaluate_solver(p4_gen, mdptoolbox.mdp.ValueIterationGS, discount_factor, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueIterationGS seems to ignore the max_iter parameter. It is quite sensitive to a high discount factor, resulting in long computation times compared to other algorithms.\n",
    "\n",
    "#### 5. RelativeValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2x2: RelativeValueIteration(discount_factor=0.95, max_iter=1) ran for 0.0s (0.2s including initialization)\n",
      "\tgenerated data: solver score 0.701 vs greedy score 0.701\n",
      "2x2: RelativeValueIteration(discount_factor=0.95, max_iter=100) ran for 0.0s (0.2s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n",
      "2x2: RelativeValueIteration(discount_factor=0.95, max_iter=10000) ran for 2.3s (2.5s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n",
      "--------------------------------------------------------------------------------\n",
      "2x2: RelativeValueIteration(discount_factor=0.6, max_iter=100) ran for 0.0s (0.2s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n",
      "2x2: RelativeValueIteration(discount_factor=0.8, max_iter=100) ran for 0.0s (0.2s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n",
      "2x2: RelativeValueIteration(discount_factor=0.9, max_iter=100) ran for 0.0s (0.3s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n",
      "2x2: RelativeValueIteration(discount_factor=0.95, max_iter=100) ran for 0.0s (0.2s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n",
      "2x2: RelativeValueIteration(discount_factor=0.99, max_iter=100) ran for 0.0s (0.2s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n",
      "2x2: RelativeValueIteration(discount_factor=0.999, max_iter=100) ran for 0.1s (0.3s including initialization)\n",
      "\tgenerated data: solver score 0.710 vs greedy score 0.701\n"
     ]
    }
   ],
   "source": [
    "for max_iter in [1, 100, 10000]:\n",
    "    evaluate_solver(p4_gen, mdptoolbox.mdp.RelativeValueIteration, 0.95, max_iter)\n",
    "print('-' * 80)\n",
    "for discount_factor in [0.6, 0.8, 0.9, 0.95, 0.99, 0.999]:\n",
    "    evaluate_solver(p4_gen, mdptoolbox.mdp.RelativeValueIteration, discount_factor, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RelativeValueIteration is much faster than ValueIteration but converges slower. Even a high discount factor does not seem to have any impact on policy or computation time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
